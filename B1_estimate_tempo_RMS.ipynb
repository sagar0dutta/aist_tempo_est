{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from compute_tempo import *\n",
    "# from extract_dance_onsets import *\n",
    "from rms_extract_dance_onsets import *\n",
    "\n",
    "\n",
    "# from aist_pos1s_EsTempo import *\n",
    "# coco={    \n",
    "# 0: \"nose\", 1: \"left_eye\", 2: \"right_eye\", 3: \"left_ear\",4: \"right_ear\",5: \"left_shoulder\",\n",
    "# 6: \"right_shoulder\",7: \"left_elbow\",8: \"right_elbow\",9: \"left_wrist\",10: \"right_wrist\",\n",
    "# 11: \"left_hip\",12: \"right_hip\",13: \"left_knee\",14: \"right_knee\",15: \"left_ankle\",16: \"right_ankle\",}  \n",
    "\n",
    "def load_pickle(filepath):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        json_data = pickle.load(f)\n",
    "    return json_data\n",
    "\n",
    "def save_to_pickle(filepath, data):\n",
    "    # filepath = os.path.join(savepath, filename)\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "        \n",
    "def create_onset_dir(tempo_dir):\n",
    "    # main_dir = \"/itf-fi-ml/home/sagardu/aist_tempo_est/saved_result\"\n",
    "    directories = [f\"{tempo_dir}/pos\", f\"{tempo_dir}/pos/ax0\",\n",
    "                   f\"{tempo_dir}/pos/ax1\", f\"{tempo_dir}/pos/combination\",\n",
    "                   f\"{tempo_dir}/pos/resultant\",\n",
    "                   \n",
    "                   f\"{tempo_dir}/vel\", f\"{tempo_dir}/vel/ax0\",\n",
    "                   f\"{tempo_dir}/vel/ax1\", f\"{tempo_dir}/vel/combination\",\n",
    "                   f\"{tempo_dir}/vel/resultant\",]\n",
    "    \n",
    "    for dir_path in directories:\n",
    "        # full_path = os.path.join(main_dir, dir_path)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        \n",
    "# create_onset_dir(\"/itf-fi-ml/home/sagardu/aist_tempo_est/rms_extracted_body_onsets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## March 11 Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMS Onset extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1510 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1510/1510 [01:09<00:00, 21.59it/s]\n",
      "100%|██████████| 1510/1510 [00:32<00:00, 46.81it/s]\n",
      "100%|██████████| 1510/1510 [00:33<00:00, 45.50it/s]\n",
      "100%|██████████| 1510/1510 [00:32<00:00, 45.86it/s]\n",
      "100%|██████████| 1510/1510 [00:33<00:00, 45.11it/s]\n",
      "100%|██████████| 1510/1510 [00:32<00:00, 47.09it/s]\n",
      "100%|██████████| 1510/1510 [00:31<00:00, 47.81it/s]\n",
      "100%|██████████| 1510/1510 [00:32<00:00, 47.12it/s]\n"
     ]
    }
   ],
   "source": [
    "config1 = {\"sub_dir\": [\"hand\"], \"mode\": [\"zero_uni\", \"zero_bi\"], \n",
    "           \"markerA_id\": [9, 10], \"a\": 60, \"b\": 140, \"metric\": [\"pos\"]}\n",
    "config2 = {\"sub_dir\": [\"foot\"], \"mode\": [\"zero_uni\", \"zero_bi\"],\n",
    "           \"markerA_id\": [15, 16], \"a\": 60, \"b\": 140, \"metric\": [\"pos\"]}\n",
    "\n",
    "configs = [config1, config2]\n",
    "# create_onset_dir(\"/itf-fi-ml/home/sagardu/aist_tempo_est/extracted_body_onsets\", \"thres_0.4\")\n",
    "for cfg in configs:\n",
    "    a = cfg[\"a\"]\n",
    "    b = cfg[\"b\"]\n",
    "    \n",
    "    for sub_dir in cfg[\"sub_dir\"]:\n",
    "        for mode in cfg[\"mode\"]:\n",
    "            for markerA_id in cfg[\"markerA_id\"]:\n",
    "                for metric in cfg[\"metric\"]:\n",
    "                    \n",
    "                    savepath = f\"./rms_extracted_body_onsets/{metric}\"           \n",
    "                    extract_body_onsets(mode, markerA_id, savepath, h_thres = 0.1,\n",
    "                               vel_mode= \"on\" if metric == \"vel\" else \"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate Tempo - Discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1510 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1510/1510 [07:22<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total processed: 1341\n"
     ]
    }
   ],
   "source": [
    "json_filename = \"music_id_tempo.json\"\n",
    "with open(json_filename, \"r\") as file:\n",
    "    aist_tempo = json.load(file)\n",
    "    \n",
    "def create_dir(main_dir, tempo_dir):\n",
    "    # main_dir = \"/itf-fi-ml/home/sagardu/aist_tempo_est/saved_result\"\n",
    "    directories = [f\"{tempo_dir}/pos\", f\"{tempo_dir}/vel\",\n",
    "                   f\"{tempo_dir}/tempo_data/pos\", f\"{tempo_dir}/tempo_data/vel\",]\n",
    "    \n",
    "    for dir_path in directories:\n",
    "        full_path = os.path.join(main_dir, dir_path)\n",
    "        os.makedirs(full_path, exist_ok=True)\n",
    "        \n",
    "segment_keys = [\"both_hand_x\", \"both_hand_y\", \"both_foot_x\", \"both_foot_y\", \n",
    "                \"lefthand_xy\", \"righthand_xy\", \"leftfoot_xy\", \"rightfoot_xy\", \n",
    "                \"left_hand_x\", \"right_hand_x\", \"left_hand_y\", \"right_hand_y\", \n",
    "                \"left_foot_x\", \"right_foot_x\", \"left_foot_y\", \"right_foot_y\", \n",
    "                \n",
    "                \"bothhand_x_bothfoot_x\", \"bothhand_y_bothfoot_y\",\n",
    "                \"lefthand_xy_righthand_xy\", \"leftfoot_xy_rightfoot_xy\",\n",
    "                \"bothhand_x_bothhand_y\", \"bothfoot_x_bothfoot_y\",\n",
    "                \n",
    "                \"both_hand_resultant\", \"both_foot_resultant\", \"left_hand_resultant\", \n",
    "                \"right_hand_resultant\", \"left_foot_resultant\", \"right_foot_resultant\"]\n",
    "\n",
    "result = { key: {\n",
    "    \"filename\": [],\n",
    "    \"dance_genre\": [],\n",
    "    \"situation\": [],\n",
    "    \"camera_id\": [],\n",
    "    \"dancer_id\": [],\n",
    "    \"music_id\": [],\n",
    "    \"choreo_id\": [],\n",
    "    \"music_tempo\": [],\n",
    "    \"estimated_bpm_per_window\": [],\n",
    "    \"magnitude_per_window\": [],\n",
    "    \"bpm_avg\": [],\n",
    "    \"bpm_mode\": [],\n",
    "    \"bpm_median\": [],\n",
    "} for key in segment_keys }\n",
    "\n",
    "fps = 60\n",
    "w_sec = 5\n",
    "h_sec = w_sec/2\n",
    "window_size = int(fps*w_sec)\n",
    "hop_size = int(fps*h_sec)\n",
    "\n",
    "a = 60 \n",
    "b = 140\n",
    "tempi_range = np.arange(a,b,1)\n",
    "metric = \"pos\"\n",
    "mode = \"zero_uni\"\n",
    "\n",
    "main_dir = \"/itf-fi-ml/home/sagardu/aist_tempo_est/saved_result_rms\"\n",
    "create_dir(main_dir, f\"tempo_{a}_{b}\")\n",
    "\n",
    "save_dir = f\"./saved_result_rms/tempo_{a}_{b}/\"     # RMS\n",
    "onset_dir = f\"./rms_extracted_body_onsets/{metric}/\"        # RMS\n",
    "f_path = \"./aist_dataset/aist_annotation/keypoints2d\"\n",
    "aist_filelist = os.listdir(f_path)\n",
    "\n",
    "\n",
    "count= 0\n",
    "for idx, filename in enumerate(tqdm(aist_filelist)):\n",
    "    \n",
    "    file_info = filename.split(\"_\")\n",
    "    dance_genre = file_info[0] \n",
    "    situation = file_info[1] \n",
    "    camera_id = file_info[2] \n",
    "    dancer_id = file_info[3]\n",
    "    music_id = file_info[4]\n",
    "    choreo_id = file_info[5].strip(\".pkl\")\n",
    "    \n",
    "    test_path = os.path.join(onset_dir, \"ax0\", f\"left_wrist_{mode}_{filename}\")\n",
    "    isExist = os.path.exists(test_path) \n",
    "    if not isExist:\n",
    "        continue\n",
    "                            \n",
    "    left_hand_x  = load_pickle(os.path.join(onset_dir, \"ax0\", f\"left_wrist_{mode}_{filename}\"))\n",
    "    left_hand_y  = load_pickle(os.path.join(onset_dir, \"ax1\", f\"left_wrist_{mode}_{filename}\"))\n",
    "    \n",
    "    right_hand_x = load_pickle(os.path.join(onset_dir, \"ax0\", f\"right_wrist_{mode}_{filename}\"))\n",
    "    right_hand_y = load_pickle(os.path.join(onset_dir, \"ax1\", f\"right_wrist_{mode}_{filename}\"))\n",
    "    \n",
    "    left_foot_x  = load_pickle(os.path.join(onset_dir, \"ax0\", f\"left_ankle_{mode}_{filename}\"))\n",
    "    left_foot_y  = load_pickle(os.path.join(onset_dir, \"ax1\", f\"left_ankle_{mode}_{filename}\"))\n",
    "    \n",
    "    right_foot_x = load_pickle(os.path.join(onset_dir, \"ax0\", f\"right_ankle_{mode}_{filename}\"))\n",
    "    right_foot_y = load_pickle(os.path.join(onset_dir, \"ax1\", f\"right_ankle_{mode}_{filename}\"))\n",
    "    \n",
    "    novelty_length = left_hand_x['raw_signal'].shape[0]\n",
    "    \n",
    "    key = 'sensor_onsets'       #   sensor_abs_pos_filtered\n",
    "    thres = 0.2     # time threshold\n",
    "    \n",
    "    bothhand_x = filter_dir_onsets_by_threshold((left_hand_x[key] + right_hand_x[key]), threshold_s= thres, fps=fps)\n",
    "    bothhand_y = filter_dir_onsets_by_threshold((left_hand_y[key] + right_hand_y[key]), threshold_s= thres, fps=fps)\n",
    "\n",
    "    bothfoot_x = filter_dir_onsets_by_threshold((left_foot_x[key] + right_foot_x[key]), threshold_s= thres, fps=fps)\n",
    "    bothfoot_y = filter_dir_onsets_by_threshold((left_foot_y[key] + right_foot_y[key]), threshold_s= thres, fps=fps)\n",
    "    \n",
    "    lefthand_xy = filter_dir_onsets_by_threshold((left_hand_x[key] + left_hand_y[key]), threshold_s= thres, fps=fps)\n",
    "    righthand_xy = filter_dir_onsets_by_threshold((right_hand_x[key] + right_hand_y[key]), threshold_s= thres, fps=fps)\n",
    "\n",
    "    leftfoot_xy = filter_dir_onsets_by_threshold((left_foot_x[key] + left_foot_y[key]), threshold_s= thres, fps=fps)\n",
    "    rightfoot_xy = filter_dir_onsets_by_threshold((right_foot_x[key] + right_foot_y[key]), threshold_s= thres, fps=fps)\n",
    "    \n",
    "    # New combinations\n",
    "    \n",
    "    bothhand_x_bothfoot_x = filter_dir_onsets_by_threshold((bothhand_x + bothfoot_x), threshold_s= thres, fps=fps)\n",
    "    bothhand_y_bothfoot_y = filter_dir_onsets_by_threshold((bothhand_y + bothfoot_y), threshold_s= thres, fps=fps)\n",
    "    \n",
    "    lefthand_xy_righthand_xy = filter_dir_onsets_by_threshold((lefthand_xy + righthand_xy), threshold_s= thres, fps=fps)\n",
    "    leftfoot_xy_rightfoot_xy = filter_dir_onsets_by_threshold((leftfoot_xy + rightfoot_xy), threshold_s= thres, fps=fps)\n",
    "    \n",
    "    bothhand_x_bothhand_y = filter_dir_onsets_by_threshold((bothhand_x + bothhand_y), threshold_s= thres, fps=fps)\n",
    "    bothfoot_x_bothfoot_y = filter_dir_onsets_by_threshold((bothfoot_x + bothfoot_y), threshold_s= thres, fps=fps)\n",
    "    \n",
    "    \n",
    "    # Resultant part\n",
    "    key1 = 'resultant_onsets'\n",
    "    left_hand_resultant  = load_pickle(os.path.join(onset_dir, \"resultant\", f\"left_wrist_{mode}_{filename}\"))\n",
    "    right_hand_resultant  = load_pickle(os.path.join(onset_dir, \"resultant\", f\"right_wrist_{mode}_{filename}\"))\n",
    "\n",
    "    left_foot_resultant = load_pickle(os.path.join(onset_dir, \"resultant\", f\"left_ankle_{mode}_{filename}\"))\n",
    "    right_foot_resultant = load_pickle(os.path.join(onset_dir, \"resultant\", f\"right_ankle_{mode}_{filename}\"))\n",
    "    \n",
    "    both_hand_resultant = filter_dir_onsets_by_threshold((left_hand_resultant[key1] + right_hand_resultant[key1]), threshold_s= thres, fps=fps)\n",
    "    both_foot_resultant = filter_dir_onsets_by_threshold((left_foot_resultant[key1] + right_foot_resultant[key1]), threshold_s= thres, fps=fps)\n",
    "    \n",
    "    segment_ax = {\n",
    "                \"both_hand_x\": bothhand_x, \"both_hand_y\": bothhand_y, \"both_foot_x\": bothfoot_x, \"both_foot_y\": bothfoot_y,\n",
    "                \"lefthand_xy\": lefthand_xy, \"righthand_xy\": righthand_xy, \"leftfoot_xy\": leftfoot_xy, \"rightfoot_xy\": rightfoot_xy,\n",
    "                \n",
    "                \"left_hand_x\": left_hand_x[key], \"right_hand_x\": right_hand_x[key], \n",
    "                \"left_hand_y\": left_hand_y[key], \"right_hand_y\": right_hand_y[key],\n",
    "                \n",
    "                \"left_foot_x\": left_foot_x[key], \"right_foot_x\": right_foot_x[key],\n",
    "                \"left_foot_y\": left_foot_y[key], \"right_foot_y\": right_foot_y[key],\n",
    "                \n",
    "                \"bothhand_x_bothfoot_x\": bothhand_x_bothfoot_x, \"bothhand_y_bothfoot_y\": bothhand_y_bothfoot_y,\n",
    "                \"lefthand_xy_righthand_xy\": lefthand_xy_righthand_xy, \"leftfoot_xy_rightfoot_xy\": leftfoot_xy_rightfoot_xy,\n",
    "                \"bothhand_x_bothhand_y\": bothhand_x_bothhand_y, \"bothfoot_x_bothfoot_y\": bothfoot_x_bothfoot_y,\n",
    "                \n",
    "                \n",
    "                \"both_hand_resultant\": both_hand_resultant, \"both_foot_resultant\": both_foot_resultant,                         \n",
    "                \"left_hand_resultant\": left_hand_resultant[key1], \"right_hand_resultant\": right_hand_resultant[key1],\n",
    "                \"left_foot_resultant\": left_foot_resultant[key1], \"right_foot_resultant\": right_foot_resultant[key1],\n",
    "                }\n",
    "    tempo_data = {}\n",
    "    for seg_key, seg in segment_ax.items():\n",
    "        \n",
    "        sensor_onsets = binary_to_peak(seg, peak_duration=0.05)\n",
    "        \n",
    "        tempogram_ab, tempogram_raw, time_axis_seconds, tempo_axis_bpm = compute_tempogram(sensor_onsets, fps, \n",
    "                                                                        window_length=window_size, hop_size=hop_size, tempi=tempi_range)\n",
    "        \n",
    "\n",
    "        tempo_data_maxmethod = dance_beat_tempo_estimation_maxmethod(tempogram_ab, tempogram_raw, fps, \n",
    "                                                        novelty_length, window_size, hop_size, tempi_range)\n",
    "    \n",
    "        tempo_data[seg_key] = tempo_data_maxmethod\n",
    "        \n",
    "        estimated_bpm_per_window = tempo_data_maxmethod[\"bpm_arr\"]\n",
    "        magnitude_per_window = tempo_data_maxmethod[\"mag_arr\"]\n",
    "        \n",
    "        tempo_avg = np.round(np.average(estimated_bpm_per_window), 2)     # mean\n",
    "        tempo_mode = stats.mode(estimated_bpm_per_window.flatten())[0]        # \n",
    "        tempo_median = np.median(estimated_bpm_per_window.flatten())\n",
    "\n",
    "        # Append the rows to the DataFrame\n",
    "        result[seg_key][\"filename\"].append(filename.strip(\".pkl\"))\n",
    "        result[seg_key][\"dance_genre\"].append(dance_genre)\n",
    "        result[seg_key][\"situation\"].append(situation)\n",
    "        result[seg_key][\"camera_id\"].append(camera_id)\n",
    "        result[seg_key][\"dancer_id\"].append(dancer_id)\n",
    "        result[seg_key][\"music_id\"].append(music_id)\n",
    "        result[seg_key][\"choreo_id\"].append(choreo_id)\n",
    "        result[seg_key][\"music_tempo\"].append(aist_tempo[music_id])\n",
    "        result[seg_key][\"estimated_bpm_per_window\"].append(estimated_bpm_per_window)\n",
    "        result[seg_key][\"magnitude_per_window\"].append(magnitude_per_window)\n",
    "        result[seg_key][\"bpm_avg\"].append(tempo_avg)\n",
    "        result[seg_key][\"bpm_mode\"].append(tempo_mode)\n",
    "        result[seg_key][\"bpm_median\"].append(tempo_median)\n",
    "\n",
    "    \n",
    "    count +=1\n",
    "print(\"total processed:\",count)    \n",
    "for seg_key in segment_keys:\n",
    "    \n",
    "    fname1 = f\"{metric}/{seg_key}_{mode}_W{w_sec}_H{h_sec}_{a}_{b}.pkl\"\n",
    "    fpath1 = os.path.join(save_dir, fname1)\n",
    "    df_seg = pd.DataFrame(result[seg_key])\n",
    "    df_seg.to_pickle(fpath1)\n",
    "    \n",
    "    # tempodata_fname = f\"tempo_data/{metric}/{seg_key}_{mode}_W{w_sec}_H{h_sec}_{a}_{b}_tempo_data.pkl\"\n",
    "    # fpath2 = os.path.join(save_dir, tempodata_fname)\n",
    "    # save_to_pickle(fpath2, tempo_data[seg_key])\n",
    "#     print(f\"Saved {fname1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate Tempo - Tempogram combination method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMS estimate tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1510/1510 [04:21<00:00,  5.78it/s]\n"
     ]
    }
   ],
   "source": [
    "json_filename = \"music_id_tempo.json\"\n",
    "with open(json_filename, \"r\") as file:\n",
    "    aist_tempo = json.load(file)\n",
    "    \n",
    "def create_dir(main_dir, tempo_dir):\n",
    "    # main_dir = \"/itf-fi-ml/home/sagardu/aist_tempo_est/saved_result\"\n",
    "    directories = [f\"{tempo_dir}/pos\", f\"{tempo_dir}/vel\",\n",
    "                   f\"{tempo_dir}/tempo_data/pos\", f\"{tempo_dir}/tempo_data/vel\",]\n",
    "    \n",
    "    for dir_path in directories:\n",
    "        full_path = os.path.join(main_dir, dir_path)\n",
    "        os.makedirs(full_path, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#  Peak-based adaptive weights (2-axis version)\n",
    "# ------------------------------------------------------------\n",
    "def adaptive_axis_weights_by_peak(tempograms_abs):\n",
    "    \"\"\"\n",
    "    tempograms_abs : list/tuple of abs-valued tempograms, same shape\n",
    "    Returns        : weights that sum to 1.0\n",
    "    \"\"\"\n",
    "    peaks = np.array([np.max(t) for t in tempograms_abs], dtype=float)\n",
    "    peaks = np.nan_to_num(peaks, nan=0.0)           # safety\n",
    "    total = peaks.sum()\n",
    "    if total <= 0:\n",
    "        return np.ones_like(peaks) / len(peaks)     # uniform fallback\n",
    "    return peaks / total\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def adaptive_axis_weights_by_peak_framewise(tempograms_abs):\n",
    "    \"\"\"\n",
    "    Frame-wise adaptive weights based on per-frame peaks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tempograms_abs : list of np.ndarray\n",
    "        Each tempogram has shape (freq_bins, time_frames).\n",
    "        All must have the same shape.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    weights : np.ndarray\n",
    "        Shape (len(tempograms_abs), time_frames).\n",
    "        At each time frame, weights across axes sum to 1.0.\n",
    "    \"\"\"\n",
    "    # Stack: shape (num_axes, freq_bins, time_frames)\n",
    "    T = np.stack(tempograms_abs, axis=0)\n",
    "\n",
    "    # Per-frame peak per axis → shape (num_axes, time_frames)\n",
    "    peaks = np.max(T, axis=1)\n",
    "\n",
    "    # Safety replace NaN\n",
    "    peaks = np.nan_to_num(peaks, nan=0.0)\n",
    "\n",
    "    # Normalize frame-wise\n",
    "    totals = np.sum(peaks, axis=0, keepdims=True)  # (1, time_frames)\n",
    "    totals[totals <= 0] = 1.0  # avoid div by 0\n",
    "    weights = peaks / totals\n",
    "\n",
    "    return weights\n",
    "\n",
    "        \n",
    "segment_keys = ['adaptv_Bhandfoot_y', \"adaptv_LRfoot_xy\", \n",
    "                \"adaptv_LRhand_xy\",  \"adaptv_Bfoot_x_y\",\n",
    "                \"adaptv_Bhandfoot_x\", \"adaptv_Bhand_x_y\",\n",
    "                \n",
    "                \"adaptv_LRfoot_res\", \"adaptv_LRhand_res\", \"adaptv_Bhandfoot_res\",\n",
    "                ]\n",
    "\n",
    "result ={ key:\n",
    "    {\"filename\": [],\n",
    "    \"dance_genre\": [],\n",
    "    \"situation\": [],\n",
    "    \"camera_id\": [],\n",
    "    \"dancer_id\": [],\n",
    "    \"music_id\": [],\n",
    "    \"choreo_id\": [],\n",
    "    \"music_tempo\": [],\n",
    "    \"estimated_bpm_per_window\": [],\n",
    "    \"magnitude_per_window\": [],\n",
    "    \"bpm_avg\": [],\n",
    "    \"bpm_mode\": [],\n",
    "    \"bpm_median\": [],\n",
    "} for key in segment_keys }\n",
    "\n",
    "fps = 60\n",
    "w_sec = 5\n",
    "h_sec = w_sec/2\n",
    "window_size = int(fps*w_sec)\n",
    "hop_size = int(fps*h_sec)\n",
    "\n",
    "a = 60; b =140\n",
    "tempi_range = np.arange(a,b,1)\n",
    "metric = \"pos\"\n",
    "mode = \"zero_bi\"\n",
    "\n",
    "main_dir = \"/itf-fi-ml/home/sagardu/aist_tempo_est/saved_result_rms_adaptive\"\n",
    "create_dir(main_dir, f\"tempo_{a}_{b}\")\n",
    "\n",
    "save_dir = f\"./saved_result_rms_adaptive/tempo_{a}_{b}/\"\n",
    "f_path = \"./aist_dataset/aist_annotation/keypoints2d\"\n",
    "aist_filelist = os.listdir(f_path)\n",
    "\n",
    "pos_onset_dir = f\"./rms_extracted_body_onsets/pos/\"\n",
    "# vel_onset_dir = f\"./extracted_body_onsets/vel/\"\n",
    "\n",
    "\n",
    "count= 0\n",
    "for idx, filename in enumerate(tqdm(aist_filelist)):\n",
    "    count +=1\n",
    "    file_info = filename.split(\"_\")\n",
    "    dance_genre = file_info[0] \n",
    "    situation = file_info[1] \n",
    "    camera_id = file_info[2] \n",
    "    dancer_id = file_info[3]\n",
    "    music_id = file_info[4]\n",
    "    choreo_id = file_info[5].strip(\".pkl\")\n",
    "    \n",
    "    test_path = os.path.join(pos_onset_dir, \"ax0\", f\"left_wrist_{mode}_{filename}\")\n",
    "    isExist = os.path.exists(test_path) \n",
    "    if not isExist:\n",
    "        continue\n",
    "    \n",
    "    left_hand_x  = load_pickle(os.path.join(pos_onset_dir, \"ax0\", f\"left_wrist_{mode}_{filename}\"))\n",
    "    left_hand_y  = load_pickle(os.path.join(pos_onset_dir, \"ax1\", f\"left_wrist_{mode}_{filename}\"))\n",
    "\n",
    "    right_hand_x = load_pickle(os.path.join(pos_onset_dir, \"ax0\", f\"right_wrist_{mode}_{filename}\"))\n",
    "    right_hand_y = load_pickle(os.path.join(pos_onset_dir, \"ax1\", f\"right_wrist_{mode}_{filename}\"))\n",
    "\n",
    "    left_foot_x  = load_pickle(os.path.join(pos_onset_dir, \"ax0\", f\"left_ankle_{mode}_{filename}\"))\n",
    "    left_foot_y  = load_pickle(os.path.join(pos_onset_dir, \"ax1\", f\"left_ankle_{mode}_{filename}\"))\n",
    "\n",
    "    right_foot_x = load_pickle(os.path.join(pos_onset_dir, \"ax0\", f\"right_ankle_{mode}_{filename}\"))\n",
    "    right_foot_y = load_pickle(os.path.join(pos_onset_dir, \"ax1\", f\"right_ankle_{mode}_{filename}\"))\n",
    "\n",
    "    novelty_length = left_hand_x['raw_signal'].shape[0]\n",
    "\n",
    "\n",
    "    key = 'sensor_onsets'  # or 'sensor_abs_vel_filtered', depending on your data\n",
    "    thres = 0.2            # time threshold\n",
    "\n",
    "    # Position-based filtered onsets\n",
    "    bothhand_x = filter_dir_onsets_by_threshold((left_hand_x[key] + right_hand_x[key]), threshold_s=thres, fps=fps)\n",
    "    bothhand_y = filter_dir_onsets_by_threshold((left_hand_y[key] + right_hand_y[key]), threshold_s=thres, fps=fps)\n",
    "\n",
    "    bothfoot_x = filter_dir_onsets_by_threshold((left_foot_x[key] + right_foot_x[key]), threshold_s=thres, fps=fps)\n",
    "    bothfoot_y = filter_dir_onsets_by_threshold((left_foot_y[key] + right_foot_y[key]), threshold_s=thres, fps=fps)\n",
    "\n",
    "    lefthand_xy = filter_dir_onsets_by_threshold((left_hand_x[key] + left_hand_y[key]), threshold_s=thres, fps=fps)\n",
    "    righthand_xy = filter_dir_onsets_by_threshold((right_hand_x[key] + right_hand_y[key]), threshold_s=thres, fps=fps)\n",
    "\n",
    "    leftfoot_xy = filter_dir_onsets_by_threshold((left_foot_x[key] + left_foot_y[key]), threshold_s=thres, fps=fps)\n",
    "    rightfoot_xy = filter_dir_onsets_by_threshold((right_foot_x[key] + right_foot_y[key]), threshold_s=thres, fps=fps)\n",
    "\n",
    "    \n",
    "    # Resultant part\n",
    "    key1 = 'resultant_onsets'\n",
    "    left_hand_resultant  = load_pickle(os.path.join(pos_onset_dir, \"resultant\", f\"left_wrist_{mode}_{filename}\"))\n",
    "    right_hand_resultant  = load_pickle(os.path.join(pos_onset_dir, \"resultant\", f\"right_wrist_{mode}_{filename}\"))\n",
    "\n",
    "    left_foot_resultant = load_pickle(os.path.join(pos_onset_dir, \"resultant\", f\"left_ankle_{mode}_{filename}\"))\n",
    "    right_foot_resultant = load_pickle(os.path.join(pos_onset_dir, \"resultant\", f\"right_ankle_{mode}_{filename}\"))\n",
    "    \n",
    "    both_hand_resultant = filter_dir_onsets_by_threshold((left_hand_resultant[key1] + right_hand_resultant[key1]), threshold_s= thres, fps=fps)\n",
    "    both_foot_resultant = filter_dir_onsets_by_threshold((left_foot_resultant[key1] + right_foot_resultant[key1]), threshold_s= thres, fps=fps)\n",
    "\n",
    "    \n",
    "    adap_map = {\n",
    "    '1': [bothhand_y,          bothfoot_y,          'adaptv_Bhandfoot_y'],\n",
    "    '2': [leftfoot_xy,         rightfoot_xy,        'adaptv_LRfoot_xy'],\n",
    "    '3': [left_foot_resultant, right_foot_resultant,'adaptv_LRfoot_res'],\n",
    "    '4': [lefthand_xy,         righthand_xy,        'adaptv_LRhand_xy'],\n",
    "    '5': [left_hand_resultant, right_hand_resultant,'adaptv_LRhand_res'],\n",
    "    '6': [bothfoot_x,          bothfoot_y,          'adaptv_Bfoot_x_y'],\n",
    "    '7': [bothhand_x,          bothfoot_x,          'adaptv_Bhandfoot_x'],\n",
    "    '8': [bothhand_x,          bothhand_y,          'adaptv_Bhand_x_y'],\n",
    "    '9': [both_hand_resultant, both_foot_resultant, 'adaptv_Bhandfoot_res'],\n",
    "}\n",
    "       \n",
    "    # nid = \"8\"\n",
    "    for nid, _ in adap_map.items():\n",
    "        data_pair, tag = adap_map[nid][0:2], adap_map[nid][2]\n",
    "        ###########################################################################################    \n",
    "        sensor_onsets1 = binary_to_peak(data_pair[0], peak_duration=0.05)\n",
    "        sensor_onsets2 = binary_to_peak(data_pair[1], peak_duration=0.05)\n",
    "        \n",
    "        # fname1 = f\"{metric}/{tag}_{mode}_W{w_sec}_H{h_sec}_{a}_{b}.pkl\"\n",
    "\n",
    "        \n",
    "        # Compute tempograms for each sensor onset sequence\n",
    "        tempogram_ab1, tempogram_raw1, _, _ = compute_tempogram(sensor_onsets1, fps, \n",
    "                                            window_length=window_size, hop_size=hop_size, tempi=tempi_range)\n",
    "\n",
    "        tempogram_ab2, tempogram_raw2, _, _ = compute_tempogram(sensor_onsets2, fps, \n",
    "                                            window_length=window_size, hop_size=hop_size, tempi=tempi_range)\n",
    "        \n",
    "        \n",
    "        tempograms_raw = [tempogram_raw1[0], tempogram_raw2[0]]     \n",
    "        tempograms_abs = [tempogram_ab1[0], tempogram_ab2[0]]    # abs of tempogram_raw\n",
    "\n",
    "        # w_x, w_y = adaptive_axis_weights_by_peak(tempograms_abs)  # gives two weights\n",
    "        # tempogram_ab_comb = [w_x * tempograms_abs[0] + w_y * tempograms_abs[1]]\n",
    "        # tempogram_raw_comb = [w_x * tempograms_raw[0] + w_y * tempograms_raw[1]]\n",
    "        \n",
    "        weights = adaptive_axis_weights_by_peak_framewise(tempograms_abs)  \n",
    "        tempogram_ab_comb = [                           # Weighted combination frame-wise\n",
    "            weights[0, None, :] * tempograms_abs[0] +\n",
    "            weights[1, None, :] * tempograms_abs[1]\n",
    "        ]\n",
    "        \n",
    "        tempogram_raw_comb = [                          # Weighted combination frame-wise\n",
    "            weights[0, None, :] * tempograms_raw[0] +\n",
    "            weights[1, None, :] * tempograms_raw[1]\n",
    "        ]\n",
    "        \n",
    "\n",
    "        tempo_data_maxmethod = dance_beat_tempo_estimation_maxmethod(tempogram_ab_comb, tempogram_raw_comb, fps, \n",
    "                                                        novelty_length, window_size, hop_size, tempi_range)\n",
    "        \n",
    "        \n",
    "\n",
    "        #############################################################################################\n",
    "        estimated_bpm_per_window = tempo_data_maxmethod[\"bpm_arr\"]\n",
    "        magnitude_per_window = tempo_data_maxmethod[\"mag_arr\"]\n",
    "        \n",
    "        tempo_avg = np.round(np.average(estimated_bpm_per_window), 2)     # mean\n",
    "        tempo_mode = stats.mode(estimated_bpm_per_window.flatten())[0]        # \n",
    "        tempo_median = np.median(estimated_bpm_per_window.flatten())\n",
    "\n",
    "        # Append the rows to the DataFrame\n",
    "        result[tag][\"filename\"].append(filename.strip(\".pkl\"))\n",
    "        result[tag][\"dance_genre\"].append(dance_genre)\n",
    "        result[tag][\"situation\"].append(situation)\n",
    "        result[tag][\"camera_id\"].append(camera_id)\n",
    "        result[tag][\"dancer_id\"].append(dancer_id)\n",
    "        result[tag][\"music_id\"].append(music_id)\n",
    "        result[tag][\"choreo_id\"].append(choreo_id)\n",
    "        result[tag][\"music_tempo\"].append(aist_tempo[music_id])\n",
    "        result[tag][\"estimated_bpm_per_window\"].append(estimated_bpm_per_window)\n",
    "        result[tag][\"magnitude_per_window\"].append(magnitude_per_window)\n",
    "        result[tag][\"bpm_avg\"].append(tempo_avg)\n",
    "        result[tag][\"bpm_mode\"].append(tempo_mode)\n",
    "        result[tag][\"bpm_median\"].append(tempo_median)\n",
    "\n",
    "\n",
    "    # fpath1 = os.path.join(save_dir, fname1)\n",
    "    # df_seg = pd.DataFrame(result)\n",
    "    # df_seg.to_pickle(fpath1)\n",
    "    \n",
    "for seg_key in segment_keys:\n",
    "    \n",
    "    fname1 = f\"{metric}/{seg_key}_{mode}_W{w_sec}_H{h_sec}_{a}_{b}.pkl\"\n",
    "    fpath1 = os.path.join(save_dir, fname1)\n",
    "    df_seg = pd.DataFrame(result[seg_key])\n",
    "    df_seg.to_pickle(fpath1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using raw input to tempogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1510 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1510/1510 [02:54<00:00,  8.64it/s]\n"
     ]
    }
   ],
   "source": [
    "json_filename = \"music_id_tempo.json\"\n",
    "with open(json_filename, \"r\") as file:\n",
    "    aist_tempo = json.load(file)\n",
    "    \n",
    "def create_dir(main_dir, tempo_dir):\n",
    "    # main_dir = \"/itf-fi-ml/home/sagardu/aist_tempo_est/saved_result\"\n",
    "    directories = [f\"{tempo_dir}/pos\", f\"{tempo_dir}/vel\",\n",
    "                   f\"{tempo_dir}/tempo_data/pos\", f\"{tempo_dir}/tempo_data/vel\",]\n",
    "    \n",
    "    for dir_path in directories:\n",
    "        full_path = os.path.join(main_dir, dir_path)\n",
    "        os.makedirs(full_path, exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#  Peak-based adaptive weights (2-axis version)\n",
    "# ------------------------------------------------------------\n",
    "def adaptive_axis_weights_by_peak(tempograms_abs):\n",
    "    \"\"\"\n",
    "    tempograms_abs : list/tuple of abs-valued tempograms, same shape\n",
    "    Returns        : weights that sum to 1.0\n",
    "    \"\"\"\n",
    "    peaks = np.array([np.max(t) for t in tempograms_abs], dtype=float)\n",
    "    peaks = np.nan_to_num(peaks, nan=0.0)           # safety\n",
    "    total = peaks.sum()\n",
    "    if total <= 0:\n",
    "        return np.ones_like(peaks) / len(peaks)     # uniform fallback\n",
    "    return peaks / total\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def adaptive_axis_weights_by_peak_framewise(tempograms_abs):\n",
    "    \"\"\"\n",
    "    Frame-wise adaptive weights based on per-frame peaks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tempograms_abs : list of np.ndarray\n",
    "        Each tempogram has shape (freq_bins, time_frames).\n",
    "        All must have the same shape.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    weights : np.ndarray\n",
    "        Shape (len(tempograms_abs), time_frames).\n",
    "        At each time frame, weights across axes sum to 1.0.\n",
    "    \"\"\"\n",
    "    # Stack: shape (num_axes, freq_bins, time_frames)\n",
    "    T = np.stack(tempograms_abs, axis=0)\n",
    "\n",
    "    # Per-frame peak per axis → shape (num_axes, time_frames)\n",
    "    peaks = np.max(T, axis=1)\n",
    "\n",
    "    # Safety replace NaN\n",
    "    peaks = np.nan_to_num(peaks, nan=0.0)\n",
    "\n",
    "    # Normalize frame-wise\n",
    "    totals = np.sum(peaks, axis=0, keepdims=True)  # (1, time_frames)\n",
    "    totals[totals <= 0] = 1.0  # avoid div by 0\n",
    "    weights = peaks / totals\n",
    "\n",
    "    return weights\n",
    "\n",
    "        \n",
    "segment_keys = ['adaptv_Bhandfoot_y', \"adaptv_LRfoot_xy\", \n",
    "                \"adaptv_LRhand_xy\",  \"adaptv_Bfoot_x_y\",\n",
    "                \"adaptv_Bhandfoot_x\", \"adaptv_Bhand_x_y\",\n",
    "                \n",
    "                \"adaptv_LRfoot_res\", \"adaptv_LRhand_res\", \"adaptv_Bhandfoot_res\",\n",
    "                ]\n",
    "\n",
    "result ={ key:\n",
    "    {\"filename\": [],\n",
    "    \"dance_genre\": [],\n",
    "    \"situation\": [],\n",
    "    \"camera_id\": [],\n",
    "    \"dancer_id\": [],\n",
    "    \"music_id\": [],\n",
    "    \"choreo_id\": [],\n",
    "    \"music_tempo\": [],\n",
    "    \"estimated_bpm_per_window\": [],\n",
    "    \"magnitude_per_window\": [],\n",
    "    \"bpm_avg\": [],\n",
    "    \"bpm_mode\": [],\n",
    "    \"bpm_median\": [],\n",
    "} for key in segment_keys }\n",
    "\n",
    "fps = 60\n",
    "w_sec = 5\n",
    "h_sec = w_sec/2\n",
    "window_size = int(fps*w_sec)\n",
    "hop_size = int(fps*h_sec)\n",
    "\n",
    "a = 60; b =140\n",
    "tempi_range = np.arange(a,b,1)\n",
    "metric = \"pos\"\n",
    "mode = \"zero_bi\"\n",
    "\n",
    "main_dir = \"/itf-fi-ml/home/sagardu/aist_tempo_est/saved_result_rms_adaptive\"\n",
    "create_dir(main_dir, f\"tempo_{a}_{b}\")\n",
    "\n",
    "save_dir = f\"./saved_result_rms_adaptive/tempo_{a}_{b}/\"\n",
    "f_path = \"./aist_dataset/aist_annotation/keypoints2d\"\n",
    "aist_filelist = os.listdir(f_path)\n",
    "\n",
    "pos_onset_dir = f\"./rms_extracted_body_onsets/pos/\"\n",
    "# vel_onset_dir = f\"./extracted_body_onsets/vel/\"\n",
    "\n",
    "\n",
    "count= 0\n",
    "for idx, filename in enumerate(tqdm(aist_filelist)):\n",
    "    count +=1\n",
    "    file_info = filename.split(\"_\")\n",
    "    dance_genre = file_info[0] \n",
    "    situation = file_info[1] \n",
    "    camera_id = file_info[2] \n",
    "    dancer_id = file_info[3]\n",
    "    music_id = file_info[4]\n",
    "    choreo_id = file_info[5].strip(\".pkl\")\n",
    "    \n",
    "    test_path = os.path.join(pos_onset_dir, \"ax0\", f\"left_wrist_{mode}_{filename}\")\n",
    "    isExist = os.path.exists(test_path) \n",
    "    if not isExist:\n",
    "        continue\n",
    "    \n",
    "    left_hand_x  = load_pickle(os.path.join(pos_onset_dir, \"ax0\", f\"left_wrist_{mode}_{filename}\"))\n",
    "    left_hand_y  = load_pickle(os.path.join(pos_onset_dir, \"ax1\", f\"left_wrist_{mode}_{filename}\"))\n",
    "\n",
    "    right_hand_x = load_pickle(os.path.join(pos_onset_dir, \"ax0\", f\"right_wrist_{mode}_{filename}\"))\n",
    "    right_hand_y = load_pickle(os.path.join(pos_onset_dir, \"ax1\", f\"right_wrist_{mode}_{filename}\"))\n",
    "\n",
    "    left_foot_x  = load_pickle(os.path.join(pos_onset_dir, \"ax0\", f\"left_ankle_{mode}_{filename}\"))\n",
    "    left_foot_y  = load_pickle(os.path.join(pos_onset_dir, \"ax1\", f\"left_ankle_{mode}_{filename}\"))\n",
    "\n",
    "    right_foot_x = load_pickle(os.path.join(pos_onset_dir, \"ax0\", f\"right_ankle_{mode}_{filename}\"))\n",
    "    right_foot_y = load_pickle(os.path.join(pos_onset_dir, \"ax1\", f\"right_ankle_{mode}_{filename}\"))\n",
    "\n",
    "    novelty_length = left_hand_x['raw_signal'].shape[0]\n",
    "\n",
    "\n",
    "    key = 'sensor_abs_pos_norm'  # or 'sensor_abs_vel_filtered', depending on your data\n",
    "    thres = 0.2            # time threshold\n",
    "\n",
    "    # Position-based filtered onsets\n",
    "    bothhand_x = left_hand_x[key] + right_hand_x[key]\n",
    "    bothhand_y = left_hand_y[key] + right_hand_y[key]\n",
    "\n",
    "    bothfoot_x = left_foot_x[key] + right_foot_x[key]\n",
    "    bothfoot_y = left_foot_y[key] + right_foot_y[key]\n",
    "\n",
    "    lefthand_xy = left_hand_x[key] + left_hand_y[key]\n",
    "    righthand_xy = right_hand_x[key] + right_hand_y[key]\n",
    "\n",
    "    leftfoot_xy = left_foot_x[key] + left_foot_y[key]\n",
    "    rightfoot_xy = right_foot_x[key] + right_foot_y[key]\n",
    "\n",
    "    \n",
    "    # Resultant part\n",
    "    key1 = 'resultant_onsets'\n",
    "    left_hand_resultant  = load_pickle(os.path.join(pos_onset_dir, \"resultant\", f\"left_wrist_{mode}_{filename}\"))\n",
    "    right_hand_resultant  = load_pickle(os.path.join(pos_onset_dir, \"resultant\", f\"right_wrist_{mode}_{filename}\"))\n",
    "\n",
    "    left_foot_resultant = load_pickle(os.path.join(pos_onset_dir, \"resultant\", f\"left_ankle_{mode}_{filename}\"))\n",
    "    right_foot_resultant = load_pickle(os.path.join(pos_onset_dir, \"resultant\", f\"right_ankle_{mode}_{filename}\"))\n",
    "    \n",
    "    both_hand_resultant = filter_dir_onsets_by_threshold((left_hand_resultant[key1] + right_hand_resultant[key1]), threshold_s= thres, fps=fps)\n",
    "    both_foot_resultant = filter_dir_onsets_by_threshold((left_foot_resultant[key1] + right_foot_resultant[key1]), threshold_s= thres, fps=fps)\n",
    "\n",
    "    \n",
    "    adap_map = {\n",
    "    '1': [bothhand_y,          bothfoot_y,          'adaptv_Bhandfoot_y'],\n",
    "    '2': [leftfoot_xy,         rightfoot_xy,        'adaptv_LRfoot_xy'],\n",
    "    '3': [left_foot_resultant[key1], right_foot_resultant[key1],'adaptv_LRfoot_res'],\n",
    "    '4': [lefthand_xy,         righthand_xy,        'adaptv_LRhand_xy'],\n",
    "    '5': [left_hand_resultant[key1], right_hand_resultant[key1],'adaptv_LRhand_res'],\n",
    "    '6': [bothfoot_x,          bothfoot_y,          'adaptv_Bfoot_x_y'],\n",
    "    '7': [bothhand_x,          bothfoot_x,          'adaptv_Bhandfoot_x'],\n",
    "    '8': [bothhand_x,          bothhand_y,          'adaptv_Bhand_x_y'],\n",
    "    '9': [both_hand_resultant, both_foot_resultant, 'adaptv_Bhandfoot_res'],\n",
    "        }\n",
    "       \n",
    "    # nid = \"8\"\n",
    "    for nid, _ in adap_map.items():\n",
    "        data_pair, tag = adap_map[nid][0:2], adap_map[nid][2]\n",
    "        ###########################################################################################    \n",
    "        sensor_onsets1 = data_pair[0]\n",
    "        sensor_onsets2 = data_pair[1]\n",
    "        \n",
    "        # fname1 = f\"{metric}/{tag}_{mode}_W{w_sec}_H{h_sec}_{a}_{b}.pkl\"\n",
    "\n",
    "        \n",
    "        # Compute tempograms for each sensor onset sequence\n",
    "        tempogram_ab1, tempogram_raw1, _, _ = compute_tempogram(sensor_onsets1.reshape(-1,1), fps, \n",
    "                                            window_length=window_size, hop_size=hop_size, tempi=tempi_range)\n",
    "\n",
    "        tempogram_ab2, tempogram_raw2, _, _ = compute_tempogram(sensor_onsets2.reshape(-1,1), fps, \n",
    "                                            window_length=window_size, hop_size=hop_size, tempi=tempi_range)\n",
    "        \n",
    "        \n",
    "        tempograms_raw = [tempogram_raw1[0], tempogram_raw2[0]]     \n",
    "        tempograms_abs = [tempogram_ab1[0], tempogram_ab2[0]]    # abs of tempogram_raw\n",
    "\n",
    "        # w_x, w_y = adaptive_axis_weights_by_peak(tempograms_abs)  # gives two weights\n",
    "        # tempogram_ab_comb = [w_x * tempograms_abs[0] + w_y * tempograms_abs[1]]\n",
    "        # tempogram_raw_comb = [w_x * tempograms_raw[0] + w_y * tempograms_raw[1]]\n",
    "        \n",
    "        weights = adaptive_axis_weights_by_peak_framewise(tempograms_abs)  \n",
    "        tempogram_ab_comb = [                           # Weighted combination frame-wise\n",
    "            weights[0, None, :] * tempograms_abs[0] +\n",
    "            weights[1, None, :] * tempograms_abs[1]\n",
    "        ]\n",
    "        \n",
    "        tempogram_raw_comb = [                          # Weighted combination frame-wise\n",
    "            weights[0, None, :] * tempograms_raw[0] +\n",
    "            weights[1, None, :] * tempograms_raw[1]\n",
    "        ]\n",
    "        \n",
    "\n",
    "        tempo_data_maxmethod = dance_beat_tempo_estimation_maxmethod(tempogram_ab_comb, tempogram_raw_comb, fps, \n",
    "                                                        novelty_length, window_size, hop_size, tempi_range)\n",
    "        \n",
    "        \n",
    "\n",
    "        #############################################################################################\n",
    "        estimated_bpm_per_window = tempo_data_maxmethod[\"bpm_arr\"]\n",
    "        magnitude_per_window = tempo_data_maxmethod[\"mag_arr\"]\n",
    "        \n",
    "        tempo_avg = np.round(np.average(estimated_bpm_per_window), 2)     # mean\n",
    "        tempo_mode = stats.mode(estimated_bpm_per_window.flatten())[0]        # \n",
    "        tempo_median = np.median(estimated_bpm_per_window.flatten())\n",
    "\n",
    "        # Append the rows to the DataFrame\n",
    "        result[tag][\"filename\"].append(filename.strip(\".pkl\"))\n",
    "        result[tag][\"dance_genre\"].append(dance_genre)\n",
    "        result[tag][\"situation\"].append(situation)\n",
    "        result[tag][\"camera_id\"].append(camera_id)\n",
    "        result[tag][\"dancer_id\"].append(dancer_id)\n",
    "        result[tag][\"music_id\"].append(music_id)\n",
    "        result[tag][\"choreo_id\"].append(choreo_id)\n",
    "        result[tag][\"music_tempo\"].append(aist_tempo[music_id])\n",
    "        result[tag][\"estimated_bpm_per_window\"].append(estimated_bpm_per_window)\n",
    "        result[tag][\"magnitude_per_window\"].append(magnitude_per_window)\n",
    "        result[tag][\"bpm_avg\"].append(tempo_avg)\n",
    "        result[tag][\"bpm_mode\"].append(tempo_mode)\n",
    "        result[tag][\"bpm_median\"].append(tempo_median)\n",
    "\n",
    "\n",
    "    # fpath1 = os.path.join(save_dir, fname1)\n",
    "    # df_seg = pd.DataFrame(result)\n",
    "    # df_seg.to_pickle(fpath1)\n",
    "    \n",
    "for seg_key in segment_keys:\n",
    "    \n",
    "    fname1 = f\"{metric}/{seg_key}_{mode}_W{w_sec}_H{h_sec}_{a}_{b}.pkl\"\n",
    "    fpath1 = os.path.join(save_dir, fname1)\n",
    "    df_seg = pd.DataFrame(result[seg_key])\n",
    "    df_seg.to_pickle(fpath1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
