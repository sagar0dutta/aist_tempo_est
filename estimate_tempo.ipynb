{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from compute_tempo import *\n",
    "from extract_dance_onsets import *\n",
    "# from aist_pos1s_EsTempo import *\n",
    "# coco={    \n",
    "# 0: \"nose\", 1: \"left_eye\", 2: \"right_eye\", 3: \"left_ear\",4: \"right_ear\",5: \"left_shoulder\",\n",
    "# 6: \"right_shoulder\",7: \"left_elbow\",8: \"right_elbow\",9: \"left_wrist\",10: \"right_wrist\",\n",
    "# 11: \"left_hip\",12: \"right_hip\",13: \"left_knee\",14: \"right_knee\",15: \"left_ankle\",16: \"right_ankle\",}  \n",
    "\n",
    "def load_pickle(filepath):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        json_data = pickle.load(f)\n",
    "    return json_data\n",
    "\n",
    "def save_to_pickle(filepath, data):\n",
    "    # filepath = os.path.join(savepath, filename)\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "        \n",
    "def create_onset_dir(main_dir, tempo_dir):\n",
    "    # main_dir = \"/itf-fi-ml/home/sagardu/aist_tempo_est/saved_result\"\n",
    "    directories = [f\"{tempo_dir}/pos\", f\"{tempo_dir}/pos/ax0\",\n",
    "                   f\"{tempo_dir}/pos/ax1\", f\"{tempo_dir}/pos/combination\",\n",
    "                   f\"{tempo_dir}/pos/resultant\",\n",
    "                   \n",
    "                   f\"{tempo_dir}/vel\", f\"{tempo_dir}/vel/ax0\",\n",
    "                   f\"{tempo_dir}/vel/ax1\", f\"{tempo_dir}/vel/combination\",\n",
    "                   f\"{tempo_dir}/vel/resultant\",]\n",
    "    \n",
    "    for dir_path in directories:\n",
    "        full_path = os.path.join(main_dir, dir_path)\n",
    "        os.makedirs(full_path, exist_ok=True)\n",
    "        \n",
    "# create_onset_dir(\"/itf-fi-ml/home/sagardu/aist_tempo_est/extracted_body_onsets\", \"thres_0.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## March 11 Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onset Extraction Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# marker_dict = {9: \"left_wrist\", 10: \"right_wrist\", \n",
    "#                 15: \"left_ankle\", 16: \"right_ankle\", \n",
    "#                 }   # 11: \"left_hip\",12: \"right_hip\"\n",
    "\n",
    "config1 = {\"sub_dir\": [\"hand\"], \"mode\": [\"zero_uni\", \"zero_bi\"], \n",
    "           \"markerA_id\": [9, 10], \"a\": 60, \"b\": 140, \"metric\": [\"pos\", \"vel\"]}\n",
    "config2 = {\"sub_dir\": [\"foot\"], \"mode\": [\"zero_uni\", \"zero_bi\"],\n",
    "           \"markerA_id\": [15, 16], \"a\": 60, \"b\": 140, \"metric\": [\"pos\", \"vel\"]}\n",
    "\n",
    "configs = [config1, config2]\n",
    "# create_onset_dir(\"/itf-fi-ml/home/sagardu/aist_tempo_est/extracted_body_onsets\", \"thres_0.4\")\n",
    "for cfg in configs:\n",
    "    a = cfg[\"a\"]\n",
    "    b = cfg[\"b\"]\n",
    "    \n",
    "    for sub_dir in cfg[\"sub_dir\"]:\n",
    "        for mode in cfg[\"mode\"]:\n",
    "            for markerA_id in cfg[\"markerA_id\"]:\n",
    "                for metric in cfg[\"metric\"]:\n",
    "                    \n",
    "                    savepath = f\"./extracted_body_onsets/{metric}\"           \n",
    "                    extract_body_onsets(mode, markerA_id, savepath, h_thres = 0.1,\n",
    "                               vel_mode= \"on\" if metric == \"vel\" else \"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1510/1510 [01:08<00:00, 22.11it/s]\n",
      "100%|██████████| 1510/1510 [01:08<00:00, 22.11it/s]\n",
      "100%|██████████| 1510/1510 [01:08<00:00, 22.13it/s]\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 1510/1510 [01:08<00:00, 22.12it/s]\n",
      "\n",
      "\n",
      "100%|██████████| 1510/1510 [01:08<00:00, 22.11it/s]\n",
      "\n",
      "100%|██████████| 1510/1510 [01:08<00:00, 22.11it/s]\n",
      "\n",
      "\n",
      "100%|██████████| 1510/1510 [01:08<00:00, 22.11it/s]\n",
      "100%|██████████| 1510/1510 [01:08<00:00, 22.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "config1 = {\"sub_dir\": [\"hand\"], \"mode\": [\"zero_uni\", \"zero_bi\"], \n",
    "           \"markerA_id\": [9, 10], \"a\": 60, \"b\": 140, \"metric\": [\"pos\", \"vel\"]}\n",
    "config2 = {\"sub_dir\": [\"foot\"], \"mode\": [\"zero_uni\", \"zero_bi\"],\n",
    "           \"markerA_id\": [15, 16], \"a\": 60, \"b\": 140, \"metric\": [\"pos\", \"vel\"]}\n",
    "configs = [config1, config2]\n",
    "\n",
    "# Build a list of all tasks\n",
    "tasks = []\n",
    "for cfg in configs:\n",
    "    for sub_dir, mode, markerA_id, metric in product(\n",
    "        cfg[\"sub_dir\"], cfg[\"mode\"], cfg[\"markerA_id\"], cfg[\"metric\"]\n",
    "    ):\n",
    "        savepath = f\"./extracted_body_onsets/{metric}\"\n",
    "        vel_mode = \"on\" if metric == \"vel\" else \"off\"\n",
    "        tasks.append((mode, markerA_id, savepath, vel_mode))\n",
    "\n",
    "# Worker wrapper\n",
    "def _run_extraction(args):\n",
    "    mode, markerA_id, savepath, vel_mode = args\n",
    "    extract_body_onsets(mode, markerA_id, savepath, h_thres=0.1, vel_mode=vel_mode)\n",
    "\n",
    "# Parallel execution\n",
    "with ProcessPoolExecutor() as pool:\n",
    "    pool.map(_run_extraction, tasks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = 60\n",
    "a = 70; b =140\n",
    "\n",
    "metric = \"pos\"\n",
    "mode = \"zero_uni\"\n",
    "\n",
    "\n",
    "onset_dir = f\"./extracted_body_onsets/{metric}\"\n",
    "save_dir = f\"./extracted_body_onsets/{metric}/combination\"\n",
    "f_path = \"./aist_dataset/aist_annotation/keypoints2d\"\n",
    "aist_filelist = os.listdir(f_path)\n",
    "\n",
    "\n",
    "for idx, filename in enumerate(tqdm(aist_filelist)):\n",
    "\n",
    "    \n",
    "    test_path = os.path.join(onset_dir, \"ax0\", f\"left_wrist_{mode}_{filename}\")\n",
    "    isExist = os.path.exists(test_path) \n",
    "    if not isExist:\n",
    "        continue\n",
    "                            \n",
    "    left_hand_x  = load_pickle(os.path.join(onset_dir, \"ax0\", f\"left_wrist_{mode}_{filename}\"))\n",
    "    left_hand_y  = load_pickle(os.path.join(onset_dir, \"ax1\", f\"left_wrist_{mode}_{filename}\"))\n",
    "    \n",
    "    right_hand_x = load_pickle(os.path.join(onset_dir, \"ax0\", f\"right_wrist_{mode}_{filename}\"))\n",
    "    right_hand_y = load_pickle(os.path.join(onset_dir, \"ax1\", f\"right_wrist_{mode}_{filename}\"))\n",
    "    \n",
    "    left_foot_x  = load_pickle(os.path.join(onset_dir, \"ax0\", f\"left_ankle_{mode}_{filename}\"))\n",
    "    left_foot_y  = load_pickle(os.path.join(onset_dir, \"ax1\", f\"left_ankle_{mode}_{filename}\"))\n",
    "    \n",
    "    right_foot_x = load_pickle(os.path.join(onset_dir, \"ax0\", f\"right_ankle_{mode}_{filename}\"))\n",
    "    right_foot_y = load_pickle(os.path.join(onset_dir, \"ax1\", f\"right_ankle_{mode}_{filename}\"))\n",
    "    \n",
    "    novelty_length = left_hand_x['raw_signal'].shape[0]\n",
    "    \n",
    "    key = 'sensor_onsets'\n",
    "    thres = 0.25\n",
    "    \n",
    "    bothhand_x = filter_dir_onsets_by_threshold((left_hand_x[key] + right_hand_x[key]), threshold_s= thres, fps=fps)\n",
    "    bothhand_y = filter_dir_onsets_by_threshold((left_hand_y[key] + right_hand_y[key]), threshold_s= thres, fps=fps)\n",
    "\n",
    "    bothfoot_x = filter_dir_onsets_by_threshold((left_foot_x[key] + right_foot_x[key]), threshold_s= thres, fps=fps)\n",
    "    bothfoot_y = filter_dir_onsets_by_threshold((left_foot_y[key] + right_foot_y[key]), threshold_s= thres, fps=fps)\n",
    "    \n",
    "    lefthand_xy = filter_dir_onsets_by_threshold((left_hand_x[key] + left_hand_y[key]), threshold_s= thres, fps=fps)\n",
    "    righthand_xy = filter_dir_onsets_by_threshold((right_hand_x[key] + right_hand_y[key]), threshold_s= thres, fps=fps)\n",
    "\n",
    "    leftfoot_xy = filter_dir_onsets_by_threshold((left_foot_x[key] + left_foot_y[key]), threshold_s= thres, fps=fps)\n",
    "    rightfoot_xy = filter_dir_onsets_by_threshold((right_foot_x[key] + right_foot_y[key]), threshold_s= thres, fps=fps)\n",
    "    \n",
    "    # Resultant part\n",
    "    key1 = 'resultant_onsets'\n",
    "    left_hand_resultant  = load_pickle(os.path.join(onset_dir, \"resultant\", f\"left_wrist_{mode}_{filename}\"))\n",
    "    right_hand_resultant  = load_pickle(os.path.join(onset_dir, \"resultant\", f\"right_wrist_{mode}_{filename}\"))\n",
    "\n",
    "    left_foot_resultant = load_pickle(os.path.join(onset_dir, \"resultant\", f\"left_ankle_{mode}_{filename}\"))\n",
    "    right_foot_resultant = load_pickle(os.path.join(onset_dir, \"resultant\", f\"right_ankle_{mode}_{filename}\"))\n",
    "    \n",
    "    both_hand_resultant = filter_dir_onsets_by_threshold((left_hand_resultant[key1] + right_hand_resultant[key1]), threshold_s= thres, fps=fps)\n",
    "    both_foot_resultant = filter_dir_onsets_by_threshold((left_foot_resultant[key1] + right_foot_resultant[key1]), threshold_s= thres, fps=fps)\n",
    "    \n",
    "    \n",
    "    json_combi = {\n",
    "        \"bothhand_x\": bothhand_x,\n",
    "        \"bothhand_y\": bothhand_y,\n",
    "        \"lefthand_xy\": lefthand_xy,\n",
    "        \"righthand_xy\": righthand_xy,\n",
    "        \"bothfoot_x\": bothfoot_x,\n",
    "        \"bothfoot_y\": bothfoot_y,\n",
    "        \"leftfoot_xy\": leftfoot_xy,\n",
    "        \"rightfoot_xy\": rightfoot_xy,\n",
    "        \n",
    "        \"both_hand_resultant\": both_hand_resultant,\n",
    "        \"both_foot_resultant\": both_foot_resultant\n",
    "    }\n",
    "    \n",
    "    \n",
    "    sv_fname = f\"{filename}_{mode}_combi_onsets.pkl\"\n",
    "    fpath2 = os.path.join(save_dir, sv_fname)\n",
    "    save_to_pickle(fpath2, json_combi)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate Tempo - Discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1510/1510 [09:27<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total processed: 1341\n"
     ]
    }
   ],
   "source": [
    "json_filename = \"music_id_tempo.json\"\n",
    "with open(json_filename, \"r\") as file:\n",
    "    aist_tempo = json.load(file)\n",
    "    \n",
    "def create_dir(main_dir, tempo_dir):\n",
    "    # main_dir = \"/itf-fi-ml/home/sagardu/aist_tempo_est/saved_result\"\n",
    "    directories = [f\"{tempo_dir}/pos\", f\"{tempo_dir}/vel\",\n",
    "                   f\"{tempo_dir}/tempo_data/pos\", f\"{tempo_dir}/tempo_data/vel\",]\n",
    "    \n",
    "    for dir_path in directories:\n",
    "        full_path = os.path.join(main_dir, dir_path)\n",
    "        os.makedirs(full_path, exist_ok=True)\n",
    "        \n",
    "segment_keys = [\"both_hand_x\", \"both_hand_y\", \"both_foot_x\", \"both_foot_y\", \n",
    "                \"lefthand_xy\", \"righthand_xy\", \"leftfoot_xy\", \"rightfoot_xy\", \n",
    "                \"left_hand_x\", \"right_hand_x\", \"left_hand_y\", \"right_hand_y\", \n",
    "                \"left_foot_x\", \"right_foot_x\", \"left_foot_y\", \"right_foot_y\", \n",
    "                \"both_hand_resultant\", \"both_foot_resultant\", \"left_hand_resultant\", \n",
    "                \"right_hand_resultant\", \"left_foot_resultant\", \"right_foot_resultant\"]\n",
    "\n",
    "result = { key: {\n",
    "    \"filename\": [],\n",
    "    \"dance_genre\": [],\n",
    "    \"situation\": [],\n",
    "    \"camera_id\": [],\n",
    "    \"dancer_id\": [],\n",
    "    \"music_id\": [],\n",
    "    \"choreo_id\": [],\n",
    "    \"music_tempo\": [],\n",
    "    \"estimated_bpm_per_window\": [],\n",
    "    \"magnitude_per_window\": [],\n",
    "    \"bpm_avg\": [],\n",
    "    \"bpm_mode\": [],\n",
    "    \"bpm_median\": [],\n",
    "} for key in segment_keys }\n",
    "\n",
    "fps = 60\n",
    "w_sec = 5\n",
    "h_sec = w_sec/2\n",
    "window_size = int(fps*w_sec)\n",
    "hop_size = int(fps*h_sec)\n",
    "\n",
    "a = 30; b =140\n",
    "tempi_range = np.arange(a,b,1)\n",
    "metric = \"vel\"\n",
    "mode = \"zero_uni\"\n",
    "\n",
    "main_dir = \"/itf-fi-ml/home/sagardu/aist_tempo_est/saved_result\"\n",
    "create_dir(main_dir, f\"tempo_{a}_{b}\")\n",
    "\n",
    "save_dir = f\"./saved_result/tempo_{a}_{b}/\"\n",
    "onset_dir = f\"./extracted_body_onsets/{metric}/\"\n",
    "f_path = \"./aist_dataset/aist_annotation/keypoints2d\"\n",
    "aist_filelist = os.listdir(f_path)\n",
    "\n",
    "\n",
    "count= 0\n",
    "for idx, filename in enumerate(tqdm(aist_filelist)):\n",
    "    \n",
    "    file_info = filename.split(\"_\")\n",
    "    dance_genre = file_info[0] \n",
    "    situation = file_info[1] \n",
    "    camera_id = file_info[2] \n",
    "    dancer_id = file_info[3]\n",
    "    music_id = file_info[4]\n",
    "    choreo_id = file_info[5].strip(\".pkl\")\n",
    "    \n",
    "    test_path = os.path.join(onset_dir, \"ax0\", f\"left_wrist_{mode}_{filename}\")\n",
    "    isExist = os.path.exists(test_path) \n",
    "    if not isExist:\n",
    "        continue\n",
    "                            \n",
    "    left_hand_x  = load_pickle(os.path.join(onset_dir, \"ax0\", f\"left_wrist_{mode}_{filename}\"))\n",
    "    left_hand_y  = load_pickle(os.path.join(onset_dir, \"ax1\", f\"left_wrist_{mode}_{filename}\"))\n",
    "    \n",
    "    right_hand_x = load_pickle(os.path.join(onset_dir, \"ax0\", f\"right_wrist_{mode}_{filename}\"))\n",
    "    right_hand_y = load_pickle(os.path.join(onset_dir, \"ax1\", f\"right_wrist_{mode}_{filename}\"))\n",
    "    \n",
    "    left_foot_x  = load_pickle(os.path.join(onset_dir, \"ax0\", f\"left_ankle_{mode}_{filename}\"))\n",
    "    left_foot_y  = load_pickle(os.path.join(onset_dir, \"ax1\", f\"left_ankle_{mode}_{filename}\"))\n",
    "    \n",
    "    right_foot_x = load_pickle(os.path.join(onset_dir, \"ax0\", f\"right_ankle_{mode}_{filename}\"))\n",
    "    right_foot_y = load_pickle(os.path.join(onset_dir, \"ax1\", f\"right_ankle_{mode}_{filename}\"))\n",
    "    \n",
    "    novelty_length = left_hand_x['raw_signal'].shape[0]\n",
    "    \n",
    "    key = 'sensor_onsets'       #   sensor_abs_pos_filtered\n",
    "    thres = 0.3     # time threshold\n",
    "    \n",
    "    bothhand_x = filter_dir_onsets_by_threshold((left_hand_x[key] + right_hand_x[key]), threshold_s= thres, fps=fps)\n",
    "    bothhand_y = filter_dir_onsets_by_threshold((left_hand_y[key] + right_hand_y[key]), threshold_s= thres, fps=fps)\n",
    "\n",
    "    bothfoot_x = filter_dir_onsets_by_threshold((left_foot_x[key] + right_foot_x[key]), threshold_s= thres, fps=fps)\n",
    "    bothfoot_y = filter_dir_onsets_by_threshold((left_foot_y[key] + right_foot_y[key]), threshold_s= thres, fps=fps)\n",
    "    \n",
    "    lefthand_xy = filter_dir_onsets_by_threshold((left_hand_x[key] + left_hand_y[key]), threshold_s= thres, fps=fps)\n",
    "    righthand_xy = filter_dir_onsets_by_threshold((right_hand_x[key] + right_hand_y[key]), threshold_s= thres, fps=fps)\n",
    "\n",
    "    leftfoot_xy = filter_dir_onsets_by_threshold((left_foot_x[key] + left_foot_y[key]), threshold_s= thres, fps=fps)\n",
    "    rightfoot_xy = filter_dir_onsets_by_threshold((right_foot_x[key] + right_foot_y[key]), threshold_s= thres, fps=fps)\n",
    "    \n",
    "    # Resultant part\n",
    "    key1 = 'resultant_onsets'\n",
    "    left_hand_resultant  = load_pickle(os.path.join(onset_dir, \"resultant\", f\"left_wrist_{mode}_{filename}\"))\n",
    "    right_hand_resultant  = load_pickle(os.path.join(onset_dir, \"resultant\", f\"right_wrist_{mode}_{filename}\"))\n",
    "\n",
    "    left_foot_resultant = load_pickle(os.path.join(onset_dir, \"resultant\", f\"left_ankle_{mode}_{filename}\"))\n",
    "    right_foot_resultant = load_pickle(os.path.join(onset_dir, \"resultant\", f\"right_ankle_{mode}_{filename}\"))\n",
    "    \n",
    "    both_hand_resultant = filter_dir_onsets_by_threshold((left_hand_resultant[key1] + right_hand_resultant[key1]), threshold_s= thres, fps=fps)\n",
    "    both_foot_resultant = filter_dir_onsets_by_threshold((left_foot_resultant[key1] + right_foot_resultant[key1]), threshold_s= thres, fps=fps)\n",
    "    \n",
    "    segment_ax = {\n",
    "                \"both_hand_x\": bothhand_x, \"both_hand_y\": bothhand_y, \"both_foot_x\": bothfoot_x, \"both_foot_y\": bothfoot_y,\n",
    "                \"lefthand_xy\": lefthand_xy, \"righthand_xy\": righthand_xy, \"leftfoot_xy\": leftfoot_xy, \"rightfoot_xy\": rightfoot_xy,\n",
    "                \n",
    "                \"left_hand_x\": left_hand_x[key], \"right_hand_x\": right_hand_x[key], \n",
    "                \"left_hand_y\": left_hand_y[key], \"right_hand_y\": right_hand_y[key],\n",
    "                \n",
    "                \"left_foot_x\": left_foot_x[key], \"right_foot_x\": right_foot_x[key],\n",
    "                \"left_foot_y\": left_foot_y[key], \"right_foot_y\": right_foot_y[key],\n",
    "                \n",
    "                \"both_hand_resultant\": both_hand_resultant, \"both_foot_resultant\": both_foot_resultant,                         \n",
    "                \"left_hand_resultant\": left_hand_resultant[key1], \"right_hand_resultant\": right_hand_resultant[key1],\n",
    "                \"left_foot_resultant\": left_foot_resultant[key1], \"right_foot_resultant\": right_foot_resultant[key1],\n",
    "                }\n",
    "    tempo_data = {}\n",
    "    for seg_key, seg in segment_ax.items():\n",
    "        \n",
    "        sensor_onsets = binary_to_peak(seg, peak_duration=0.05)\n",
    "        \n",
    "        tempogram_ab, tempogram_raw, time_axis_seconds, tempo_axis_bpm = compute_tempogram(sensor_onsets, fps, \n",
    "                                                                        window_length=window_size, hop_size=hop_size, tempi=tempi_range)\n",
    "        \n",
    "\n",
    "        tempo_data_maxmethod = dance_beat_tempo_estimation_maxmethod(tempogram_ab, tempogram_raw, fps, \n",
    "                                                        novelty_length, window_size, hop_size, tempi_range)\n",
    "    \n",
    "        tempo_data[seg_key] = tempo_data_maxmethod\n",
    "        \n",
    "        estimated_bpm_per_window = tempo_data_maxmethod[\"bpm_arr\"]\n",
    "        magnitude_per_window = tempo_data_maxmethod[\"mag_arr\"]\n",
    "        \n",
    "        tempo_avg = np.round(np.average(estimated_bpm_per_window), 2)     # mean\n",
    "        tempo_mode = stats.mode(estimated_bpm_per_window.flatten())[0]        # \n",
    "        tempo_median = np.median(estimated_bpm_per_window.flatten())\n",
    "\n",
    "        # Append the rows to the DataFrame\n",
    "        result[seg_key][\"filename\"].append(filename.strip(\".pkl\"))\n",
    "        result[seg_key][\"dance_genre\"].append(dance_genre)\n",
    "        result[seg_key][\"situation\"].append(situation)\n",
    "        result[seg_key][\"camera_id\"].append(camera_id)\n",
    "        result[seg_key][\"dancer_id\"].append(dancer_id)\n",
    "        result[seg_key][\"music_id\"].append(music_id)\n",
    "        result[seg_key][\"choreo_id\"].append(choreo_id)\n",
    "        result[seg_key][\"music_tempo\"].append(aist_tempo[music_id])\n",
    "        result[seg_key][\"estimated_bpm_per_window\"].append(estimated_bpm_per_window)\n",
    "        result[seg_key][\"magnitude_per_window\"].append(magnitude_per_window)\n",
    "        result[seg_key][\"bpm_avg\"].append(tempo_avg)\n",
    "        result[seg_key][\"bpm_mode\"].append(tempo_mode)\n",
    "        result[seg_key][\"bpm_median\"].append(tempo_median)\n",
    "    \n",
    "    # tempodata_fname = f\"tempo_data/{metric}/{filename[:-4]}_{mode}_W{w_sec}_H{h_sec}_{a}_{b}_tempo_data.pkl\"\n",
    "    # fpath2 = os.path.join(save_dir, tempodata_fname)\n",
    "    # save_to_pickle(fpath2, tempo_data)\n",
    "    \n",
    "    count +=1\n",
    "print(\"total processed:\",count)    \n",
    "for seg_key in segment_keys:\n",
    "    \n",
    "    fname1 = f\"{metric}/{seg_key}_{mode}_W{w_sec}_H{h_sec}_{a}_{b}.pkl\"\n",
    "    fpath1 = os.path.join(save_dir, fname1)\n",
    "    df_seg = pd.DataFrame(result[seg_key])\n",
    "    df_seg.to_pickle(fpath1)\n",
    "    \n",
    "#     # tempodata_fname = f\"tempo_data/{metric}/{seg_key}_{mode}_W{w_sec}_H{h_sec}_{a}_{b}_tempo_data.pkl\"\n",
    "#     # fpath2 = os.path.join(save_dir, tempodata_fname)\n",
    "#     # save_to_pickle(fpath2, tempo_data[seg_key])    \n",
    "#     print(f\"Saved {fname1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(data):\n",
    "    min_vals = np.min(data)  # Minimum values along each column\n",
    "    max_vals = np.max(data)  # Maximum values along each column\n",
    "    normalized_data = (data - min_vals) / (max_vals - min_vals)\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate Tempo - Tempogram combination method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1510/1510 [01:02<00:00, 24.02it/s]\n"
     ]
    }
   ],
   "source": [
    "json_filename = \"music_id_tempo.json\"\n",
    "with open(json_filename, \"r\") as file:\n",
    "    aist_tempo = json.load(file)\n",
    "    \n",
    "def create_dir(main_dir, tempo_dir):\n",
    "    # main_dir = \"/itf-fi-ml/home/sagardu/aist_tempo_est/saved_result\"\n",
    "    directories = [f\"{tempo_dir}/pos\", f\"{tempo_dir}/vel\",\n",
    "                   f\"{tempo_dir}/tempo_data/pos\", f\"{tempo_dir}/tempo_data/vel\",]\n",
    "    \n",
    "    for dir_path in directories:\n",
    "        full_path = os.path.join(main_dir, dir_path)\n",
    "        os.makedirs(full_path, exist_ok=True)\n",
    "        \n",
    "segment_keys = [\"both_hand_x\", \"both_hand_y\", \"both_foot_x\", \"both_foot_y\", \n",
    "                \"lefthand_xy\", \"righthand_xy\", \"leftfoot_xy\", \"rightfoot_xy\", \n",
    "                \"left_hand_x\", \"right_hand_x\", \"left_hand_y\", \"right_hand_y\", \n",
    "                \"left_foot_x\", \"right_foot_x\", \"left_foot_y\", \"right_foot_y\", \n",
    "                \"both_hand_resultant\", \"both_foot_resultant\", \"left_hand_resultant\", \n",
    "                \"right_hand_resultant\", \"left_foot_resultant\", \"right_foot_resultant\"]\n",
    "\n",
    "result ={\n",
    "    \"filename\": [],\n",
    "    \"dance_genre\": [],\n",
    "    \"situation\": [],\n",
    "    \"camera_id\": [],\n",
    "    \"dancer_id\": [],\n",
    "    \"music_id\": [],\n",
    "    \"choreo_id\": [],\n",
    "    \"music_tempo\": [],\n",
    "    \"estimated_bpm_per_window\": [],\n",
    "    \"magnitude_per_window\": [],\n",
    "    \"bpm_avg\": [],\n",
    "    \"bpm_mode\": [],\n",
    "    \"bpm_median\": [],\n",
    "}\n",
    "\n",
    "fps = 60\n",
    "w_sec = 5\n",
    "h_sec = w_sec/2\n",
    "window_size = int(fps*w_sec)\n",
    "hop_size = int(fps*h_sec)\n",
    "\n",
    "a = 60; b =140\n",
    "tempi_range = np.arange(a,b,1)\n",
    "metric = \"pos\"\n",
    "mode = \"zero_uni\"\n",
    "\n",
    "main_dir = \"/itf-fi-ml/home/sagardu/aist_tempo_est/saved_result\"\n",
    "create_dir(main_dir, f\"tttempo_{a}_{b}\")\n",
    "\n",
    "save_dir = f\"./saved_result/tttempo_{a}_{b}/\"\n",
    "f_path = \"./aist_dataset/aist_annotation/keypoints2d\"\n",
    "aist_filelist = os.listdir(f_path)\n",
    "\n",
    "pos_onset_dir = f\"./extracted_body_onsets/pos/\"\n",
    "vel_onset_dir = f\"./extracted_body_onsets/vel/\"\n",
    "\n",
    "\n",
    "count= 0\n",
    "for idx, filename in enumerate(tqdm(aist_filelist)):\n",
    "    \n",
    "    file_info = filename.split(\"_\")\n",
    "    dance_genre = file_info[0] \n",
    "    situation = file_info[1] \n",
    "    camera_id = file_info[2] \n",
    "    dancer_id = file_info[3]\n",
    "    music_id = file_info[4]\n",
    "    choreo_id = file_info[5].strip(\".pkl\")\n",
    "    \n",
    "    test_path = os.path.join(pos_onset_dir, \"ax0\", f\"left_wrist_{mode}_{filename}\")\n",
    "    isExist = os.path.exists(test_path) \n",
    "    if not isExist:\n",
    "        continue\n",
    "    \n",
    "    left_hand_x  = load_pickle(os.path.join(pos_onset_dir, \"ax0\", f\"left_wrist_{mode}_{filename}\"))\n",
    "    left_hand_y  = load_pickle(os.path.join(pos_onset_dir, \"ax1\", f\"left_wrist_{mode}_{filename}\"))\n",
    "\n",
    "    right_hand_x = load_pickle(os.path.join(pos_onset_dir, \"ax0\", f\"right_wrist_{mode}_{filename}\"))\n",
    "    right_hand_y = load_pickle(os.path.join(pos_onset_dir, \"ax1\", f\"right_wrist_{mode}_{filename}\"))\n",
    "\n",
    "    left_foot_x  = load_pickle(os.path.join(pos_onset_dir, \"ax0\", f\"left_ankle_{mode}_{filename}\"))\n",
    "    left_foot_y  = load_pickle(os.path.join(pos_onset_dir, \"ax1\", f\"left_ankle_{mode}_{filename}\"))\n",
    "\n",
    "    right_foot_x = load_pickle(os.path.join(pos_onset_dir, \"ax0\", f\"right_ankle_{mode}_{filename}\"))\n",
    "    right_foot_y = load_pickle(os.path.join(pos_onset_dir, \"ax1\", f\"right_ankle_{mode}_{filename}\"))\n",
    "\n",
    "    novelty_length = left_hand_x['raw_signal'].shape[0]\n",
    "\n",
    "    #### Velocity part ####\n",
    "\n",
    "    v_left_hand_x  = load_pickle(os.path.join(vel_onset_dir, \"ax0\", f\"left_wrist_{mode}_{filename}\"))\n",
    "    v_left_hand_y  = load_pickle(os.path.join(vel_onset_dir, \"ax1\", f\"left_wrist_{mode}_{filename}\"))\n",
    "\n",
    "    v_right_hand_x = load_pickle(os.path.join(vel_onset_dir, \"ax0\", f\"right_wrist_{mode}_{filename}\"))\n",
    "    v_right_hand_y = load_pickle(os.path.join(vel_onset_dir, \"ax1\", f\"right_wrist_{mode}_{filename}\"))\n",
    "\n",
    "    v_left_foot_x  = load_pickle(os.path.join(vel_onset_dir, \"ax0\", f\"left_ankle_{mode}_{filename}\"))\n",
    "    v_left_foot_y  = load_pickle(os.path.join(vel_onset_dir, \"ax1\", f\"left_ankle_{mode}_{filename}\"))\n",
    "\n",
    "    v_right_foot_x = load_pickle(os.path.join(vel_onset_dir, \"ax0\", f\"right_ankle_{mode}_{filename}\"))\n",
    "    v_right_foot_y = load_pickle(os.path.join(vel_onset_dir, \"ax1\", f\"right_ankle_{mode}_{filename}\"))\n",
    "\n",
    "    #### Filtering Onsets ####\n",
    "\n",
    "    key = 'sensor_onsets'  # or 'sensor_abs_vel_filtered', depending on your data\n",
    "    thres = 0.3            # time threshold\n",
    "\n",
    "    # Position-based filtered onsets\n",
    "    bothhand_x = filter_dir_onsets_by_threshold((left_hand_x[key] + right_hand_x[key]), threshold_s=thres, fps=fps)\n",
    "    bothhand_y = filter_dir_onsets_by_threshold((left_hand_y[key] + right_hand_y[key]), threshold_s=thres, fps=fps)\n",
    "\n",
    "    bothfoot_x = filter_dir_onsets_by_threshold((left_foot_x[key] + right_foot_x[key]), threshold_s=thres, fps=fps)\n",
    "    bothfoot_y = filter_dir_onsets_by_threshold((left_foot_y[key] + right_foot_y[key]), threshold_s=thres, fps=fps)\n",
    "\n",
    "    lefthand_xy = filter_dir_onsets_by_threshold((left_hand_x[key] + left_hand_y[key]), threshold_s=thres, fps=fps)\n",
    "    righthand_xy = filter_dir_onsets_by_threshold((right_hand_x[key] + right_hand_y[key]), threshold_s=thres, fps=fps)\n",
    "\n",
    "    leftfoot_xy = filter_dir_onsets_by_threshold((left_foot_x[key] + left_foot_y[key]), threshold_s=thres, fps=fps)\n",
    "    rightfoot_xy = filter_dir_onsets_by_threshold((right_foot_x[key] + right_foot_y[key]), threshold_s=thres, fps=fps)\n",
    "\n",
    "    # Velocity-based filtered onsets\n",
    "    v_bothhand_x = filter_dir_onsets_by_threshold((v_left_hand_x[key] + v_right_hand_x[key]), threshold_s=thres, fps=fps)\n",
    "    v_bothhand_y = filter_dir_onsets_by_threshold((v_left_hand_y[key] + v_right_hand_y[key]), threshold_s=thres, fps=fps)\n",
    "\n",
    "    v_bothfoot_x = filter_dir_onsets_by_threshold((v_left_foot_x[key] + v_right_foot_x[key]), threshold_s=thres, fps=fps)\n",
    "    v_bothfoot_y = filter_dir_onsets_by_threshold((v_left_foot_y[key] + v_right_foot_y[key]), threshold_s=thres, fps=fps)\n",
    "\n",
    "    v_lefthand_xy = filter_dir_onsets_by_threshold((v_left_hand_x[key] + v_left_hand_y[key]), threshold_s=thres, fps=fps)\n",
    "    v_righthand_xy = filter_dir_onsets_by_threshold((v_right_hand_x[key] + v_right_hand_y[key]), threshold_s=thres, fps=fps)\n",
    "\n",
    "    v_leftfoot_xy = filter_dir_onsets_by_threshold((v_left_foot_x[key] + v_left_foot_y[key]), threshold_s=thres, fps=fps)\n",
    "    v_rightfoot_xy = filter_dir_onsets_by_threshold((v_right_foot_x[key] + v_right_foot_y[key]), threshold_s=thres, fps=fps)\n",
    "    \n",
    "\n",
    "    ###########################################################################################    \n",
    "    sensor_onsets1 = binary_to_peak(bothhand_y, peak_duration=0.05)\n",
    "    sensor_onsets2 = binary_to_peak(bothfoot_y, peak_duration=0.05)\n",
    "    \n",
    "    sensor_onsets3 = binary_to_peak(v_bothhand_y, peak_duration=0.05)   # v_bothhand_y = np.diff(bothhand_y)\n",
    "    sensor_onsets4 = binary_to_peak(v_bothfoot_y, peak_duration=0.05)\n",
    "    sensor_onsets3 = np.append(sensor_onsets3, 0).reshape(-1, 1)\n",
    "    sensor_onsets4 = np.append(sensor_onsets4, 0).reshape(-1, 1)\n",
    "    \n",
    "    # Compute tempograms for each sensor onset sequence\n",
    "    tempogram_ab1, tempogram_raw1, _, _ = compute_tempogram(sensor_onsets1, fps, \n",
    "                                        window_length=window_size, hop_size=hop_size, tempi=tempi_range)\n",
    "\n",
    "    tempogram_ab2, tempogram_raw2, _, _ = compute_tempogram(sensor_onsets2, fps, \n",
    "                                        window_length=window_size, hop_size=hop_size, tempi=tempi_range)\n",
    "\n",
    "    tempogram_ab3, tempogram_raw3, _, _ = compute_tempogram(sensor_onsets3, fps, \n",
    "                                        window_length=window_size, hop_size=hop_size, tempi=tempi_range)\n",
    "\n",
    "    tempogram_ab4, tempogram_raw4, _, _ = compute_tempogram(sensor_onsets4, fps, \n",
    "                                        window_length=window_size, hop_size=hop_size, tempi=tempi_range)\n",
    "\n",
    "    # getting accuracy 60.62%\n",
    "    # tempogram_ab = np.mean([tempogram_ab1, tempogram_ab2, tempogram_ab3, tempogram_ab4], axis=0)\n",
    "    # tempogram_raw = np.mean([tempogram_raw1, tempogram_raw2, tempogram_raw3, tempogram_raw4], axis=0)\n",
    "    \n",
    "    # Compute adaptive weights based on peak magnitudes of tempograms\n",
    "    def adaptive_weights(tempograms):\n",
    "        peak_magnitudes = np.array([np.max(t) for t in tempograms])\n",
    "        return peak_magnitudes / np.sum(peak_magnitudes)\n",
    "\n",
    "    # Tempogram lists\n",
    "    tempograms_ab = [tempogram_ab1[0], tempogram_ab2[0], tempogram_ab3[0], tempogram_ab4[0]]\n",
    "    tempograms_raw = [tempogram_raw1[0], tempogram_raw2[0], tempogram_raw3[0], tempogram_raw4[0]]\n",
    "\n",
    "    # Compute weights\n",
    "    weights_ab = adaptive_weights(tempograms_ab)\n",
    "    weights_raw = adaptive_weights(tempograms_raw)\n",
    "\n",
    "    # Compute weighted average of tempograms\n",
    "\n",
    "    # Weighted tempogram_ab\n",
    "    tempogram_ab = [sum(w * t for w, t in zip(weights_ab, tempograms_ab))]\n",
    "\n",
    "    # Weighted tempogram_raw\n",
    "    tempogram_raw = [sum(w * t for w, t in zip(weights_raw, tempograms_raw))]\n",
    "    \n",
    "    \n",
    "\n",
    "    tempo_data_maxmethod = dance_beat_tempo_estimation_maxmethod(tempogram_ab, tempogram_raw, fps, \n",
    "                                                    novelty_length, window_size, hop_size, tempi_range)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #############################################################################################\n",
    "    estimated_bpm_per_window = tempo_data_maxmethod[\"bpm_arr\"]\n",
    "    magnitude_per_window = tempo_data_maxmethod[\"mag_arr\"]\n",
    "    \n",
    "    tempo_avg = np.round(np.average(estimated_bpm_per_window), 2)     # mean\n",
    "    tempo_mode = stats.mode(estimated_bpm_per_window.flatten())[0]        # \n",
    "    tempo_median = np.median(estimated_bpm_per_window.flatten())\n",
    "\n",
    "    # Append the rows to the DataFrame\n",
    "    result[\"filename\"].append(filename.strip(\".pkl\"))\n",
    "    result[\"dance_genre\"].append(dance_genre)\n",
    "    result[\"situation\"].append(situation)\n",
    "    result[\"camera_id\"].append(camera_id)\n",
    "    result[\"dancer_id\"].append(dancer_id)\n",
    "    result[\"music_id\"].append(music_id)\n",
    "    result[\"choreo_id\"].append(choreo_id)\n",
    "    result[\"music_tempo\"].append(aist_tempo[music_id])\n",
    "    result[\"estimated_bpm_per_window\"].append(estimated_bpm_per_window)\n",
    "    result[\"magnitude_per_window\"].append(magnitude_per_window)\n",
    "    result[\"bpm_avg\"].append(tempo_avg)\n",
    "    result[\"bpm_mode\"].append(tempo_mode)\n",
    "    result[\"bpm_median\"].append(tempo_median)\n",
    "    \n",
    "    # tempodata_fname = f\"tempo_data/{metric}/{filename[:-4]}_{mode}_W{w_sec}_H{h_sec}_{a}_{b}_tempo_data.pkl\"\n",
    "    # fpath2 = os.path.join(save_dir, tempodata_fname)\n",
    "    # save_to_pickle(fpath2, tempo_data)\n",
    "    \n",
    "fname1 = f\"{metric}/bhf_{mode}_W{w_sec}_H{h_sec}_{a}_{b}.pkl\"\n",
    "fpath1 = os.path.join(save_dir, fname1)\n",
    "df_seg = pd.DataFrame(result)\n",
    "df_seg.to_pickle(fpath1)\n",
    "    \n",
    "#     # tempodata_fname = f\"tempo_data/{metric}/{seg_key}_{mode}_W{w_sec}_H{h_sec}_{a}_{b}_tempo_data.pkl\"\n",
    "#     # fpath2 = os.path.join(save_dir, tempodata_fname)\n",
    "#     # save_to_pickle(fpath2, tempo_data[seg_key])    \n",
    "#     print(f\"Saved {fname1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempogram_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### method codes ##########\n",
    "\n",
    "# getting accuracy 60.62%\n",
    "    # tempogram_ab = np.mean([tempogram_ab1, tempogram_ab2, tempogram_ab3, tempogram_ab4], axis=0)\n",
    "    # tempogram_raw = np.mean([tempogram_raw1, tempogram_raw2, tempogram_raw3, tempogram_raw4], axis=0)\n",
    "    \n",
    "    # Compute adaptive weights based on peak magnitudes of tempograms\n",
    "    # def adaptive_weights(tempograms):\n",
    "    #     peak_magnitudes = np.array([np.max(t) for t in tempograms])\n",
    "    #     return peak_magnitudes / np.sum(peak_magnitudes)\n",
    "\n",
    "    # # Tempogram lists\n",
    "    # tempograms_ab = [tempogram_ab1[0], tempogram_ab2[0], tempogram_ab3[0], tempogram_ab4[0]]\n",
    "    # tempograms_raw = [tempogram_raw1[0], tempogram_raw2[0], tempogram_raw3[0], tempogram_raw4[0]]\n",
    "\n",
    "    # # Compute weights\n",
    "    # weights_ab = adaptive_weights(tempograms_ab)\n",
    "    # weights_raw = adaptive_weights(tempograms_raw)\n",
    "\n",
    "    # # Compute weighted average of tempograms\n",
    "\n",
    "    # # Weighted tempogram_ab\n",
    "    # tempogram_ab = [sum(w * t for w, t in zip(weights_ab, tempograms_ab))]\n",
    "\n",
    "    # # Weighted tempogram_raw\n",
    "    # tempogram_raw = [sum(w * t for w, t in zip(weights_raw, tempograms_raw))]\n",
    "    \n",
    "    # -----------------------------------------------\n",
    "    # getting accuracy 58.24%\n",
    "    # tempogram_ab = np.max([tempogram_ab1, tempogram_ab2], axis=0)\n",
    "    # tempogram_raw = np.max([tempogram_raw1, tempogram_raw2], axis=0)\n",
    "    \n",
    "    # getting accuracy 54.43%\n",
    "    # tempogram_ab = np.max([tempogram_ab3, tempogram_ab4], axis=0)\n",
    "    # tempogram_raw = np.max([tempogram_raw3, tempogram_raw4], axis=0)\n",
    "    \n",
    "    # # Stability part\n",
    "    # tempogram_stack = np.stack([tempogram_ab1, tempogram_ab2, tempogram_ab3, tempogram_ab4], axis=-1)\n",
    "    # stability = np.std(tempogram_stack, axis=-1)\n",
    "    # tempogram_ab = np.mean(tempogram_stack, axis=-1) / (1 + stability)\n",
    "\n",
    "    # # Stability part\n",
    "    # tempogram_raw_stack = np.stack([tempogram_raw1, tempogram_raw2, tempogram_raw3, tempogram_raw4], axis=-1)\n",
    "    # stability_raw = np.std(tempogram_raw_stack, axis=-1)\n",
    "    # tempogram_raw = np.mean(tempogram_raw_stack, axis=-1) / (1 + stability_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
