{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from compute_tempo_aist import *\n",
    "from aist_pos1s_extraction import *\n",
    "from aist_pos1s_EsTempo import *\n",
    "# coco={    \n",
    "# 0: \"nose\", 1: \"left_eye\", 2: \"right_eye\", 3: \"left_ear\",4: \"right_ear\",5: \"left_shoulder\",\n",
    "# 6: \"right_shoulder\",7: \"left_elbow\",8: \"right_elbow\",9: \"left_wrist\",10: \"right_wrist\",\n",
    "# 11: \"left_hip\",12: \"right_hip\",13: \"left_knee\",14: \"right_knee\",15: \"left_ankle\",16: \"right_ankle\",}  \n",
    "\n",
    "def load_pickle(filepath):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        json_data = pickle.load(f)\n",
    "    return json_data\n",
    "\n",
    "def save_to_pickle(filepath, data):\n",
    "    # filepath = os.path.join(savepath, filename)\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_dict = {9: \"left_wrist\", 10: \"right_wrist\", \n",
    "                15: \"left_ankle\", 16: \"right_ankle\", \n",
    "                }   # 11: \"left_hip\",12: \"right_hip\"\n",
    "\n",
    "config1 = {\"sub_dir\": [\"hand\"], \"mode\": [\"zero_uni\", \"zero_bi\"], \n",
    "           \"markerA_id\": [9, 10], \"a\": 60, \"b\": 140, \"metric\": [\"pos\", \"vel\"]}\n",
    "config2 = {\"sub_dir\": [\"foot\"], \"mode\": [\"zero_uni\", \"zero_bi\"], \n",
    "           \"markerA_id\": [15, 16], \"a\": 60, \"b\": 140, \"metric\": [\"pos\", \"vel\"]}\n",
    "\n",
    "# config3 = {\"sub_dir\": [\"hand\"], \"mode\": [\"zero_uni\", \"zero_bi\"], \"markerA_id\": [9, 10], \"a\": 60, \"b\": 180}\n",
    "# config4 = {\"sub_dir\": [\"foot\"], \"mode\": [\"zero_uni\", \"zero_bi\"], \"markerA_id\": [15, 16], \"a\": 60, \"b\": 180}\n",
    "\n",
    "configs = [config1, config2]\n",
    "\n",
    "for cfg in configs:\n",
    "    a = cfg[\"a\"]\n",
    "    b = cfg[\"b\"]\n",
    "    # create_dir(f\"tempo_{a}_{b}\")\n",
    "    for sub_dir in cfg[\"sub_dir\"]:\n",
    "        for mode in cfg[\"mode\"]:\n",
    "            for markerA_id in cfg[\"markerA_id\"]:\n",
    "                for metric in cfg[\"metric\"]:\n",
    "                    \n",
    "                    savepath = f\"./results/one_sensor/tempo_{a}_{b}/{metric}/tempo_data\"\n",
    "                    pickle_filename = f\"./results/one_sensor/tempo_{a}_{b}/{metric}/{sub_dir}/{marker_dict[markerA_id]}_{mode}_{a}_{b}.pkl\"\n",
    "                    \n",
    "                    aist_pos1s(a, b, mode, markerA_id, pickle_filename,savepath, \n",
    "                               w_sec = 5,\n",
    "                               vel_mode= \"on\" if metric == \"vel\" else \"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## March 11 Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onset Extraction Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marker_dict = {9: \"left_wrist\", 10: \"right_wrist\", \n",
    "                15: \"left_ankle\", 16: \"right_ankle\", \n",
    "                }   # 11: \"left_hip\",12: \"right_hip\"\n",
    "\n",
    "config1 = {\"sub_dir\": [\"hand\"], \"mode\": [\"zero_uni\", \"zero_bi\"], \n",
    "           \"markerA_id\": [9, 10], \"a\": 60, \"b\": 140, \"metric\": [\"pos\", \"vel\"]}\n",
    "config2 = {\"sub_dir\": [\"foot\"], \"mode\": [\"zero_uni\", \"zero_bi\"], \n",
    "           \"markerA_id\": [15, 16], \"a\": 60, \"b\": 140, \"metric\": [\"pos\", \"vel\"]}\n",
    "\n",
    "# config3 = {\"sub_dir\": [\"hand\"], \"mode\": [\"zero_uni\", \"zero_bi\"], \"markerA_id\": [9, 10], \"a\": 60, \"b\": 180}\n",
    "# config4 = {\"sub_dir\": [\"foot\"], \"mode\": [\"zero_uni\", \"zero_bi\"], \"markerA_id\": [15, 16], \"a\": 60, \"b\": 180}\n",
    "\n",
    "configs = [config1, config2]\n",
    "\n",
    "for cfg in configs:\n",
    "    a = cfg[\"a\"]\n",
    "    b = cfg[\"b\"]\n",
    "    # create_dir(f\"tempo_{a}_{b}\")\n",
    "    for sub_dir in cfg[\"sub_dir\"]:\n",
    "        for mode in cfg[\"mode\"]:\n",
    "            for markerA_id in cfg[\"markerA_id\"]:\n",
    "                for metric in cfg[\"metric\"]:\n",
    "                    \n",
    "                    savepath = f\"./extracted_body_onsets/{metric}\"           \n",
    "                    extract_body_onsets(mode, markerA_id,savepath, \n",
    "                               vel_mode= \"on\" if metric == \"vel\" else \"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate Tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1510/1510 [04:16<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total processed: 1341\n",
      "Saved vel/both_hand_x_zero_uni_W5_H2.5_60_140.pkl\n",
      "Saved vel/both_hand_y_zero_uni_W5_H2.5_60_140.pkl\n",
      "Saved vel/both_foot_x_zero_uni_W5_H2.5_60_140.pkl\n",
      "Saved vel/both_foot_y_zero_uni_W5_H2.5_60_140.pkl\n",
      "Saved vel/lefthand_xy_zero_uni_W5_H2.5_60_140.pkl\n",
      "Saved vel/righthand_xy_zero_uni_W5_H2.5_60_140.pkl\n",
      "Saved vel/leftfoot_xy_zero_uni_W5_H2.5_60_140.pkl\n",
      "Saved vel/rightfoot_xy_zero_uni_W5_H2.5_60_140.pkl\n",
      "Saved vel/left_hand_x_zero_uni_W5_H2.5_60_140.pkl\n",
      "Saved vel/right_hand_x_zero_uni_W5_H2.5_60_140.pkl\n",
      "Saved vel/left_hand_y_zero_uni_W5_H2.5_60_140.pkl\n",
      "Saved vel/right_hand_y_zero_uni_W5_H2.5_60_140.pkl\n",
      "Saved vel/left_foot_x_zero_uni_W5_H2.5_60_140.pkl\n",
      "Saved vel/right_foot_x_zero_uni_W5_H2.5_60_140.pkl\n",
      "Saved vel/left_foot_y_zero_uni_W5_H2.5_60_140.pkl\n",
      "Saved vel/right_foot_y_zero_uni_W5_H2.5_60_140.pkl\n",
      "Saved vel/both_hand_resultant_zero_uni_W5_H2.5_60_140.pkl\n",
      "Saved vel/both_foot_resultant_zero_uni_W5_H2.5_60_140.pkl\n",
      "Saved vel/left_hand_resultant_zero_uni_W5_H2.5_60_140.pkl\n",
      "Saved vel/right_hand_resultant_zero_uni_W5_H2.5_60_140.pkl\n",
      "Saved vel/left_foot_resultant_zero_uni_W5_H2.5_60_140.pkl\n",
      "Saved vel/right_foot_resultant_zero_uni_W5_H2.5_60_140.pkl\n"
     ]
    }
   ],
   "source": [
    "json_filename = \"music_id_tempo.json\"\n",
    "with open(json_filename, \"r\") as file:\n",
    "    aist_tempo = json.load(file)\n",
    "    \n",
    "def create_dir(main_dir, tempo_dir):\n",
    "    # main_dir = \"/itf-fi-ml/home/sagardu/aist_tempo_est/saved_result\"\n",
    "    directories = [f\"{tempo_dir}/pos\", f\"{tempo_dir}/vel\",\n",
    "                   f\"{tempo_dir}/tempo_data/pos\", f\"{tempo_dir}/tempo_data/vel\",]\n",
    "    \n",
    "    for dir_path in directories:\n",
    "        full_path = os.path.join(main_dir, dir_path)\n",
    "        os.makedirs(full_path, exist_ok=True)\n",
    "        \n",
    "segment_keys = [\"both_hand_x\", \"both_hand_y\", \"both_foot_x\", \"both_foot_y\", \n",
    "                \"lefthand_xy\", \"righthand_xy\", \"leftfoot_xy\", \"rightfoot_xy\", \n",
    "                \"left_hand_x\", \"right_hand_x\", \"left_hand_y\", \"right_hand_y\", \n",
    "                \"left_foot_x\", \"right_foot_x\", \"left_foot_y\", \"right_foot_y\", \n",
    "                \"both_hand_resultant\", \"both_foot_resultant\", \"left_hand_resultant\", \n",
    "                \"right_hand_resultant\", \"left_foot_resultant\", \"right_foot_resultant\"]\n",
    "result = { key: {\n",
    "    \"filename\": [],\n",
    "    \"dance_genre\": [],\n",
    "    \"situation\": [],\n",
    "    \"camera_id\": [],\n",
    "    \"dancer_id\": [],\n",
    "    \"music_id\": [],\n",
    "    \"choreo_id\": [],\n",
    "    \"music_tempo\": [],\n",
    "    \"estimated_bpm_per_window\": [],\n",
    "    \"magnitude_per_window\": [],\n",
    "    \"bpm_avg\": [],\n",
    "    \"bpm_mode\": [],\n",
    "    \"bpm_median\": [],\n",
    "} for key in segment_keys }\n",
    "\n",
    "tempo_data = {}\n",
    "\n",
    "fps = 60\n",
    "w_sec = 5\n",
    "h_sec = w_sec/2\n",
    "window_size = int(fps*w_sec)\n",
    "hop_size = int(fps*h_sec)\n",
    "\n",
    "a = 60; b =140\n",
    "tempi_range = np.arange(a,b,1)\n",
    "metric = \"vel\"\n",
    "mode = \"zero_uni\"\n",
    "\n",
    "main_dir = \"/itf-fi-ml/home/sagardu/aist_tempo_est/saved_result\"\n",
    "create_dir(main_dir, f\"tempo_{a}_{b}\")\n",
    "\n",
    "save_dir = f\"./saved_result/tempo_{a}_{b}/\"\n",
    "onset_dir = f\"./extracted_body_onsets/{metric}/\"\n",
    "f_path = \"./aist_dataset/aist_annotation/keypoints2d\"\n",
    "aist_filelist = os.listdir(f_path)\n",
    "\n",
    "\n",
    "count= 0\n",
    "for idx, filename in enumerate(tqdm(aist_filelist)):\n",
    "    \n",
    "    file_info = filename.split(\"_\")\n",
    "    dance_genre = file_info[0] \n",
    "    situation = file_info[1] \n",
    "    camera_id = file_info[2] \n",
    "    dancer_id = file_info[3]\n",
    "    music_id = file_info[4]\n",
    "    choreo_id = file_info[5].strip(\".pkl\")\n",
    "    \n",
    "    test_path = os.path.join(onset_dir, \"ax0\", f\"left_wrist_{mode}_{filename}\")\n",
    "    isExist = os.path.exists(test_path) \n",
    "    if not isExist:\n",
    "        continue\n",
    "                            \n",
    "    left_hand_x  = load_pickle(os.path.join(onset_dir, \"ax0\", f\"left_wrist_{mode}_{filename}\"))\n",
    "    left_hand_y  = load_pickle(os.path.join(onset_dir, \"ax1\", f\"left_wrist_{mode}_{filename}\"))\n",
    "    \n",
    "    right_hand_x = load_pickle(os.path.join(onset_dir, \"ax0\", f\"right_wrist_{mode}_{filename}\"))\n",
    "    right_hand_y = load_pickle(os.path.join(onset_dir, \"ax1\", f\"right_wrist_{mode}_{filename}\"))\n",
    "    \n",
    "    left_foot_x  = load_pickle(os.path.join(onset_dir, \"ax0\", f\"left_ankle_{mode}_{filename}\"))\n",
    "    left_foot_y  = load_pickle(os.path.join(onset_dir, \"ax1\", f\"left_ankle_{mode}_{filename}\"))\n",
    "    \n",
    "    right_foot_x = load_pickle(os.path.join(onset_dir, \"ax0\", f\"right_ankle_{mode}_{filename}\"))\n",
    "    right_foot_y = load_pickle(os.path.join(onset_dir, \"ax1\", f\"right_ankle_{mode}_{filename}\"))\n",
    "    \n",
    "    novelty_length = left_hand_x['raw_signal'].shape[0]\n",
    "    \n",
    "    key = 'sensor_onsets'\n",
    "    thres = 0.2\n",
    "    \n",
    "    bothhand_x = filter_dir_onsets_by_threshold((left_hand_x[key] + right_hand_x[key]), threshold_s= thres, fps=fps)\n",
    "    bothhand_y = filter_dir_onsets_by_threshold((left_hand_y[key] + right_hand_y[key]), threshold_s= thres, fps=fps)\n",
    "\n",
    "    bothfoot_x = filter_dir_onsets_by_threshold((left_foot_x[key] + right_foot_x[key]), threshold_s= thres, fps=fps)\n",
    "    bothfoot_y = filter_dir_onsets_by_threshold((left_foot_y[key] + right_foot_y[key]), threshold_s= thres, fps=fps)\n",
    "    \n",
    "    lefthand_xy = filter_dir_onsets_by_threshold((left_hand_x[key] + left_hand_y[key]), threshold_s= thres, fps=fps)\n",
    "    righthand_xy = filter_dir_onsets_by_threshold((right_hand_x[key] + right_hand_y[key]), threshold_s= thres, fps=fps)\n",
    "\n",
    "    leftfoot_xy = filter_dir_onsets_by_threshold((left_foot_x[key] + left_foot_y[key]), threshold_s= thres, fps=fps)\n",
    "    rightfoot_xy = filter_dir_onsets_by_threshold((right_foot_x[key] + right_foot_y[key]), threshold_s= thres, fps=fps)\n",
    "    \n",
    "    # Resultant part\n",
    "    key1 = 'resultant_onsets'\n",
    "    left_hand_resultant  = load_pickle(os.path.join(onset_dir, \"resultant\", f\"left_wrist_{mode}_{filename}\"))\n",
    "    right_hand_resultant  = load_pickle(os.path.join(onset_dir, \"resultant\", f\"right_wrist_{mode}_{filename}\"))\n",
    "\n",
    "    left_foot_resultant = load_pickle(os.path.join(onset_dir, \"resultant\", f\"left_ankle_{mode}_{filename}\"))\n",
    "    right_foot_resultant = load_pickle(os.path.join(onset_dir, \"resultant\", f\"right_ankle_{mode}_{filename}\"))\n",
    "    \n",
    "    both_hand_resultant = filter_dir_onsets_by_threshold((left_hand_resultant[key1] + right_hand_resultant[key1]), threshold_s= thres, fps=fps)\n",
    "    both_foot_resultant = filter_dir_onsets_by_threshold((left_foot_resultant[key1] + right_foot_resultant[key1]), threshold_s= thres, fps=fps)\n",
    "    \n",
    "    segment_ax = {\n",
    "                \"both_hand_x\": bothhand_x, \"both_hand_y\": bothhand_y, \"both_foot_x\": bothfoot_x, \"both_foot_y\": bothfoot_y,\n",
    "                \"lefthand_xy\": lefthand_xy, \"righthand_xy\": righthand_xy, \"leftfoot_xy\": leftfoot_xy, \"rightfoot_xy\": rightfoot_xy,\n",
    "                \n",
    "                \"left_hand_x\": left_hand_x[key], \"right_hand_x\": right_hand_x[key], \n",
    "                \"left_hand_y\": left_hand_y[key], \"right_hand_y\": right_hand_y[key],\n",
    "                \n",
    "                \"left_foot_x\": left_foot_x[key], \"right_foot_x\": right_foot_x[key],\n",
    "                \"left_foot_y\": left_foot_y[key], \"right_foot_y\": right_foot_y[key],\n",
    "                \n",
    "                \"both_hand_resultant\": both_hand_resultant, \"both_foot_resultant\": both_foot_resultant,                         \n",
    "                \"left_hand_resultant\": left_hand_resultant[key1], \"right_hand_resultant\": right_hand_resultant[key1],\n",
    "                \"left_foot_resultant\": left_foot_resultant[key1], \"right_foot_resultant\": right_foot_resultant[key1],\n",
    "                }\n",
    "    \n",
    "    for seg_key, seg in segment_ax.items():\n",
    "        \n",
    "        sensor_onsets = binary_to_peak(seg, peak_duration=0.05)\n",
    "        \n",
    "        tempogram_ab, tempogram_raw, time_axis_seconds, tempo_axis_bpm = compute_tempogram(sensor_onsets, fps, \n",
    "                                                                        window_length=window_size, hop_size=hop_size, tempi=tempi_range)\n",
    "        \n",
    "\n",
    "        tempo_data_maxmethod = dance_beat_tempo_estimation_maxmethod(tempogram_ab, tempogram_raw, fps, \n",
    "                                                        novelty_length, window_size, hop_size, tempi_range)\n",
    "    \n",
    "        tempo_data[seg_key] = tempo_data_maxmethod\n",
    "        \n",
    "        estimated_bpm_per_window = tempo_data_maxmethod[\"bpm_arr\"]\n",
    "        magnitude_per_window = tempo_data_maxmethod[\"mag_arr\"]\n",
    "        \n",
    "        tempo_avg = np.round(np.average(estimated_bpm_per_window), 2)     # mean\n",
    "        tempo_mode = stats.mode(estimated_bpm_per_window.flatten())[0]        # \n",
    "        tempo_median = np.median(estimated_bpm_per_window.flatten())\n",
    "\n",
    "        # Append results for the current segment\n",
    "        result[seg_key][\"filename\"].append(filename.strip(\".pkl\"))\n",
    "        result[seg_key][\"dance_genre\"].append(dance_genre)\n",
    "        result[seg_key][\"situation\"].append(situation)\n",
    "        result[seg_key][\"camera_id\"].append(camera_id)\n",
    "        result[seg_key][\"dancer_id\"].append(dancer_id)\n",
    "        result[seg_key][\"music_id\"].append(music_id)\n",
    "        result[seg_key][\"choreo_id\"].append(choreo_id)\n",
    "        result[seg_key][\"music_tempo\"].append(aist_tempo[music_id])\n",
    "        result[seg_key][\"estimated_bpm_per_window\"].append(estimated_bpm_per_window)\n",
    "        result[seg_key][\"magnitude_per_window\"].append(magnitude_per_window)\n",
    "        result[seg_key][\"bpm_avg\"].append(tempo_avg)\n",
    "        result[seg_key][\"bpm_mode\"].append(tempo_mode)\n",
    "        result[seg_key][\"bpm_median\"].append(tempo_median)\n",
    "    count +=1\n",
    "print(\"total processed:\",count)    \n",
    "for seg_key in segment_keys:\n",
    "    \n",
    "    fname1 = f\"{metric}/{seg_key}_{mode}_W{w_sec}_H{h_sec}_{a}_{b}.pkl\"\n",
    "    fpath1 = os.path.join(save_dir, fname1)\n",
    "    df_seg = pd.DataFrame(result[seg_key])\n",
    "    df_seg.to_pickle(fpath1)\n",
    "    \n",
    "    tempodata_fname = f\"tempo_data/{metric}/{seg_key}_{mode}_W{w_sec}_H{h_sec}_{a}_{b}_tempo_data.pkl\"\n",
    "    fpath2 = os.path.join(save_dir, tempodata_fname)\n",
    "    save_to_pickle(fpath2, tempo_data[seg_key])\n",
    "    \n",
    "    print(f\"Saved {fname1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skipped Files Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Skipped_files = ['gPO_sMM_cAll_d12_mPO2_ch07.pkl', 'gBR_sBM_cAll_d04_mBR2_ch05.pkl', 'gJS_sFM_cAll_d03_mJS1_ch02.pkl', 'gMH_sMM_cAll_d24_mMH3_ch10.pkl', 'gLH_sMM_cAll_d18_mLH5_ch10.pkl', 'gJS_sMM_cAll_d02_mJS3_ch03.pkl', 'gBR_sBM_cAll_d05_mBR1_ch10.pkl', 'gPO_sMM_cAll_d11_mPO2_ch04.pkl', 'gBR_sFM_cAll_d06_mBR5_ch21.pkl', 'gJB_sMM_cAll_d09_mJB5_ch10.pkl', 'gLO_sMM_cAll_d15_mLO4_ch09.pkl', 'gBR_sMM_cAll_d06_mBR1_ch07.pkl', 'gHO_sMM_cAll_d19_mHO2_ch02.pkl', 'gBR_sBM_cAll_d04_mBR3_ch08.pkl', 'gLO_sMM_cAll_d14_mLO5_ch06.pkl', 'gJS_sFM_cAll_d03_mJS5_ch13.pkl', 'gJS_sMM_cAll_d01_mJS3_ch03.pkl', 'gLH_sMM_cAll_d17_mLH3_ch05.pkl', 'gWA_sMM_cAll_d27_mWA0_ch07.pkl', 'gKR_sMM_cAll_d30_mKR1_ch08.pkl', 'gHO_sFM_cAll_d20_mHO1_ch09.pkl', 'gBR_sBM_cAll_d05_mBR4_ch07.pkl', 'gHO_sMM_cAll_d21_mHO5_ch10.pkl', 'gBR_sBM_cAll_d04_mBR1_ch07.pkl', 'gWA_sMM_cAll_d25_mWA4_ch02.pkl', 'gLH_sMM_cAll_d17_mLH5_ch06.pkl', 'gJS_sFM_cAll_d02_mJS1_ch02.pkl', 'gPO_sMM_cAll_d11_mPO3_ch05.pkl', 'gBR_sMM_cAll_d05_mBR2_ch05.pkl', 'gMH_sMM_cAll_d23_mMH2_ch06.pkl', 'gBR_sBM_cAll_d05_mBR1_ch08.pkl', 'gBR_sBM_cAll_d05_mBR1_ch07.pkl', 'gJB_sMM_cAll_d07_mJB4_ch02.pkl', 'gBR_sBM_cAll_d06_mBR3_ch07.pkl', 'gBR_sMM_cAll_d06_mBR3_ch08.pkl', 'gBR_sBM_cAll_d05_mBR0_ch10.pkl', 'gBR_sMM_cAll_d06_mBR4_ch09.pkl', 'gJB_sMM_cAll_d07_mJB5_ch03.pkl', 'gJS_sFM_cAll_d02_mJS1_ch11.pkl', 'gLH_sMM_cAll_d16_mLH0_ch01.pkl', 'gLO_sMM_cAll_d13_mLO1_ch02.pkl', 'gJS_sFM_cAll_d01_mJS1_ch02.pkl', 'gMH_sMM_cAll_d24_mMH1_ch08.pkl', 'gBR_sBM_cAll_d04_mBR2_ch10.pkl', 'gPO_sMM_cAll_d10_mPO3_ch02.pkl', 'gKR_sFM_cAll_d30_mKR0_ch15.pkl', 'gMH_sMM_cAll_d22_mMH1_ch02.pkl', 'gBR_sMM_cAll_d05_mBR1_ch04.pkl', 'gMH_sMM_cAll_d23_mMH1_ch05.pkl', 'gBR_sBM_cAll_d05_mBR0_ch07.pkl', 'gKR_sFM_cAll_d29_mKR5_ch13.pkl', 'gJB_sMM_cAll_d07_mJB3_ch01.pkl', 'gBR_sBM_cAll_d04_mBR0_ch10.pkl', 'gJS_sFM_cAll_d02_mJS5_ch10.pkl', 'gLO_sMM_cAll_d14_mLO4_ch05.pkl', 'gJS_sFM_cAll_d03_mJS4_ch12.pkl', 'gLH_sMM_cAll_d17_mLH0_ch04.pkl', 'gLH_sMM_cAll_d16_mLH3_ch03.pkl', 'gKR_sMM_cAll_d29_mKR0_ch04.pkl', 'gJS_sFM_cAll_d01_mJS0_ch01.pkl', 'gHO_sMM_cAll_d20_mHO2_ch04.pkl', 'gLO_sMM_cAll_d13_mLO5_ch03.pkl', 'gBR_sFM_cAll_d04_mBR5_ch06.pkl', 'gKR_sMM_cAll_d30_mKR2_ch09.pkl', 'gKR_sMM_cAll_d28_mKR2_ch03.pkl', 'gHO_sMM_cAll_d20_mHO4_ch06.pkl', 'gBR_sMM_cAll_d04_mBR4_ch02.pkl', 'gBR_sBM_cAll_d05_mBR1_ch09.pkl', 'gBR_sFM_cAll_d04_mBR4_ch05.pkl', 'gHO_sMM_cAll_d19_mHO3_ch03.pkl', 'gBR_sBM_cAll_d04_mBR3_ch05.pkl', 'gBR_sBM_cAll_d06_mBR5_ch08.pkl', 'gBR_sMM_cAll_d04_mBR5_ch03.pkl', 'gBR_sFM_cAll_d06_mBR2_ch16.pkl', 'gBR_sBM_cAll_d04_mBR3_ch07.pkl', 'gLH_sMM_cAll_d18_mLH1_ch07.pkl', 'gBR_sBM_cAll_d06_mBR5_ch05.pkl', 'gHO_sFM_cAll_d19_mHO3_ch04.pkl', 'gBR_sMM_cAll_d06_mBR5_ch10.pkl', 'gJS_sMM_cAll_d02_mJS0_ch04.pkl', 'gMH_sMM_cAll_d24_mMH2_ch09.pkl', 'gHO_sMM_cAll_d20_mHO3_ch05.pkl', 'gJS_sMM_cAll_d02_mJS1_ch02.pkl', 'gPO_sMM_cAll_d12_mPO3_ch08.pkl', 'gJS_sMM_cAll_d03_mJS5_ch05.pkl', 'gBR_sBM_cAll_d06_mBR2_ch10.pkl', 'gBR_sMM_cAll_d04_mBR3_ch01.pkl', 'gJB_sFM_cAll_d08_mJB0_ch08.pkl', 'gPO_sMM_cAll_d12_mPO5_ch09.pkl', 'gJB_sMM_cAll_d09_mJB1_ch08.pkl', 'gBR_sBM_cAll_d06_mBR3_ch08.pkl', 'gMH_sFM_cAll_d22_mMH1_ch02.pkl', 'gHO_sFM_cAll_d20_mHO0_ch08.pkl', 'gBR_sBM_cAll_d05_mBR0_ch08.pkl', 'gWA_sMM_cAll_d25_mWA5_ch03.pkl', 'gBR_sFM_cAll_d04_mBR1_ch02.pkl', 'gKR_sMM_cAll_d29_mKR5_ch06.pkl', 'gJS_sMM_cAll_d03_mJS0_ch01.pkl', 'gWA_sMM_cAll_d26_mWA5_ch06.pkl', 'gJB_sFM_cAll_d07_mJB3_ch07.pkl', 'gLO_sMM_cAll_d15_mLO5_ch10.pkl', 'gLH_sMM_cAll_d18_mLH3_ch09.pkl', 'gWA_sMM_cAll_d26_mWA3_ch05.pkl', 'gBR_sFM_cAll_d04_mBR4_ch07.pkl', 'gJB_sMM_cAll_d09_mJB0_ch07.pkl', 'gBR_sBM_cAll_d05_mBR5_ch08.pkl', 'gLO_sMM_cAll_d13_mLO0_ch01.pkl', 'gLO_sMM_cAll_d15_mLO0_ch07.pkl', 'gBR_sBM_cAll_d04_mBR0_ch08.pkl', 'gPO_sMM_cAll_d10_mPO5_ch03.pkl', 'gMH_sMM_cAll_d23_mMH0_ch04.pkl', 'gJS_sMM_cAll_d03_mJS1_ch02.pkl', 'gBR_sBM_cAll_d05_mBR5_ch10.pkl', 'gBR_sBM_cAll_d04_mBR3_ch10.pkl', 'gBR_sFM_cAll_d05_mBR4_ch11.pkl', 'gLH_sMM_cAll_d18_mLH2_ch08.pkl', 'gJS_sFM_cAll_d01_mJS3_ch04.pkl', 'gWA_sMM_cAll_d27_mWA2_ch08.pkl', 'gBR_sBM_cAll_d06_mBR4_ch08.pkl', 'gJS_sFM_cAll_d01_mJS1_ch07.pkl', 'gJB_sFM_cAll_d07_mJB2_ch03.pkl', 'gKR_sMM_cAll_d28_mKR1_ch02.pkl', 'gBR_sBM_cAll_d04_mBR1_ch10.pkl', 'gHO_sFM_cAll_d20_mHO5_ch13.pkl', 'gLO_sMM_cAll_d14_mLO1_ch04.pkl', 'gJB_sFM_cAll_d09_mJB1_ch16.pkl', 'gMH_sMM_cAll_d22_mMH2_ch03.pkl', 'gBR_sFM_cAll_d04_mBR3_ch04.pkl', 'gBR_sFM_cAll_d05_mBR5_ch14.pkl', 'gPO_sMM_cAll_d12_mPO4_ch10.pkl', 'gHO_sMM_cAll_d19_mHO0_ch01.pkl', 'gWA_sMM_cAll_d25_mWA1_ch01.pkl', 'gKR_sFM_cAll_d28_mKR5_ch06.pkl', 'gKR_sMM_cAll_d29_mKR1_ch05.pkl', 'gKR_sFM_cAll_d28_mKR3_ch07.pkl', 'gKR_sMM_cAll_d28_mKR0_ch01.pkl', 'gMH_sMM_cAll_d22_mMH0_ch01.pkl', 'gHO_sMM_cAll_d21_mHO2_ch07.pkl', 'gPO_sMM_cAll_d11_mPO5_ch06.pkl', 'gJS_sFM_cAll_d02_mJS3_ch04.pkl', 'gBR_sBM_cAll_d05_mBR4_ch10.pkl', 'gBR_sMM_cAll_d05_mBR4_ch06.pkl', 'gHO_sMM_cAll_d21_mHO4_ch09.pkl', 'gHO_sFM_cAll_d20_mHO3_ch14.pkl', 'gKR_sMM_cAll_d30_mKR0_ch07.pkl', 'gWA_sMM_cAll_d27_mWA5_ch10.pkl', 'gJS_sMM_cAll_d01_mJS1_ch02.pkl', 'gBR_sBM_cAll_d04_mBR2_ch08.pkl', 'gJS_sFM_cAll_d02_mJS0_ch08.pkl', 'gJS_sMM_cAll_d01_mJS0_ch01.pkl', 'gWA_sMM_cAll_d27_mWA3_ch09.pkl', 'gLH_sMM_cAll_d16_mLH2_ch02.pkl', 'gPO_sMM_cAll_d10_mPO2_ch01.pkl', 'gJB_sFM_cAll_d09_mJB2_ch17.pkl', 'gBR_sBM_cAll_d04_mBR1_ch08.pkl', 'gLO_sMM_cAll_d15_mLO1_ch08.pkl', 'gJS_sMM_cAll_d03_mJS3_ch03.pkl', 'gWA_sMM_cAll_d26_mWA2_ch04.pkl', 'gBR_sFM_cAll_d04_mBR2_ch03.pkl', 'gBR_sBM_cAll_d05_mBR5_ch07.pkl', 'gJB_sMM_cAll_d08_mJB3_ch05.pkl', 'gJB_sMM_cAll_d09_mJB3_ch09.pkl', 'gHO_sMM_cAll_d21_mHO3_ch08.pkl', 'gBR_sBM_cAll_d05_mBR4_ch08.pkl', 'gMH_sMM_cAll_d24_mMH0_ch07.pkl', 'gBR_sFM_cAll_d05_mBR2_ch09.pkl', 'gJB_sMM_cAll_d08_mJB1_ch04.pkl', 'gJB_sMM_cAll_d08_mJB4_ch06.pkl', 'gKR_sMM_cAll_d30_mKR3_ch10.pkl']\n",
    "\n",
    "f_path = \"./aist_dataset/aist_annotation/keypoints2d\"\t\n",
    "\n",
    "for idx, filename in enumerate(tqdm(Skipped_files)):\n",
    "        \n",
    "    file_path = os.path.join(f_path, filename)\n",
    "    with open(file_path, 'rb') as file:\n",
    "            motion_data = pickle.load(file)\n",
    "\n",
    "    markerA_x = motion_data[\"keypoints2d\"][0, :, markerA_id, 0]     # array (n,)\n",
    "    markerA_y = motion_data[\"keypoints2d\"][0, :, markerA_id, 1]      # array (n,)\n",
    "    \n",
    "    print(len(markerA_x))\n",
    "    break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
